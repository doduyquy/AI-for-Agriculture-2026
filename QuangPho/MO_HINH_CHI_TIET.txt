================================================================================
    MÃ” HÃŒNH PHÃ‚N LOáº I Bá»†NH LÃšA MÃŒ - TWO-STEP WHEAT NET
    Chi tiáº¿t Kiáº¿n trÃºc & Quy trÃ¬nh Huáº¥n luyá»‡n
================================================================================

ğŸ“… NgÃ y táº¡o: 04/02/2026
ğŸ“‚ File gá»‘c: PipelineV1_Stage1_TwoStep.py
ğŸ¯ Má»¥c tiÃªu: PhÃ¢n loáº¡i 3 lá»›p bá»‡nh lÃºa mÃ¬ (Healthy, Rust, Other)


================================================================================
I. Tá»”NG QUAN MÃ” HÃŒNH
================================================================================

1. CHIáº¾N LÆ¯á»¢C PHÃ‚N LOáº I: TWO-STEP BINARY CLASSIFICATION
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Quy trÃ¬nh 2 bÆ°á»›c:                                           â”‚
   â”‚                                                             â”‚
   â”‚ STEP 1: Healthy vs Diseased (RGB-dominant)                 â”‚
   â”‚    â”œâ”€ Tá»· trá»ng: 85% RGB + 15% Spectral                    â”‚
   â”‚    â”œâ”€ Focus: PhÃ¢n biá»‡t mÃ u sáº¯c (xanh vs khÃ´ng xanh)       â”‚
   â”‚    â””â”€ Output: Binary (0=Healthy, 1=Diseased)              â”‚
   â”‚                                                             â”‚
   â”‚ STEP 2: Rust vs Other (Spectral-dominant)                  â”‚
   â”‚    â”œâ”€ Tá»· trá»ng: 30% RGB + 70% Spectral                    â”‚
   â”‚    â”œâ”€ Focus: Chá»¯ kÃ½ phá»• Ä‘áº·c trÆ°ng cho bá»‡nh gá»‰            â”‚
   â”‚    â”œâ”€ Chá»‰ Ã¡p dá»¥ng cho máº«u Diseased tá»« Step 1              â”‚
   â”‚    â””â”€ Output: Binary (0=Rust, 1=Other)                     â”‚
   â”‚                                                             â”‚
   â”‚ FINAL OUTPUT: Káº¿t há»£p xÃ¡c suáº¥t 2 bÆ°á»›c â†’ 3 classes         â”‚
   â”‚    â”œâ”€ P(Healthy) = P(Healthy | Step1)                     â”‚
   â”‚    â”œâ”€ P(Rust) = P(Diseased | Step1) Ã— P(Rust | Step2)    â”‚
   â”‚    â””â”€ P(Other) = P(Diseased | Step1) Ã— P(Other | Step2)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. Æ¯U ÄIá»‚M Cá»¦A PHÆ¯Æ NG PHÃP
   âœ“ Giáº£i quyáº¿t váº¥n Ä‘á» Healthy bá»‹ nháº§m vá»›i Rust
   âœ“ PhÃ¹ há»£p vá»›i quy trÃ¬nh cháº©n Ä‘oÃ¡n thá»±c táº¿ (táº§m soÃ¡t â†’ cháº©n Ä‘oÃ¡n)
   âœ“ Táº­n dá»¥ng Ä‘iá»ƒm máº¡nh cá»§a tá»«ng modality (RGB cho mÃ u, Spectral cho bá»‡nh)
   âœ“ Giáº£m confusion giá»¯a cÃ¡c class khÃ³ phÃ¢n biá»‡t


================================================================================
II. KIáº¾N TRÃšC MÃ” HÃŒNH CHI TIáº¾T
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          INPUT LAYER (3 Modalities)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  1. RGB Image (224Ã—224Ã—3)                                                 â”‚
â”‚     â”œâ”€ Normalize: mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] â”‚
â”‚     â””â”€ Augmentation: Flip, Rotate(Â±15Â°), Brightness/Contrast            â”‚
â”‚                                                                            â”‚
â”‚  2. Multispectral (MS) Image (224Ã—224Ã—5)                                  â”‚
â”‚     â”œâ”€ 5 bands: Blue, Green, Red, Red-Edge, NIR                         â”‚
â”‚     â””â”€ Normalize: [0, 1]                                                  â”‚
â”‚                                                                            â”‚
â”‚  3. Hyperspectral (HS) Image (224Ã—224Ã—125)                                â”‚
â”‚     â”œâ”€ 125 bands: 400-1000nm                                             â”‚
â”‚     â””â”€ Normalize: [0, 1]                                                  â”‚
â”‚                                                                            â”‚
â”‚  4. RGB Handcrafted Features (84 dims)                                    â”‚
â”‚     â”œâ”€ GLCM Texture: 36 dims (Contrast, Homogeneity, Correlation Ã— 3 ch)â”‚
â”‚     â”œâ”€ LBP Multiscale: 30 dims (R=1,2,3 on Green channel)               â”‚
â”‚     â””â”€ Color Features: 18 dims (HSV stats + ExG + VARI)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FEATURE EXTRACTION LAYER                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  BRANCH 1: RGB FEATURE EXTRACTOR (FROZEN)                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚   â””â”€ RGBCNNFeature (ResNet18 Pretrained)                                 â”‚
â”‚      â”œâ”€ Input: (B, 3, 224, 224)                                          â”‚
â”‚      â”œâ”€ Backbone: ResNet18 (ImageNet weights)                            â”‚
â”‚      â”œâ”€ Output: (B, 512, 7, 7)                                           â”‚
â”‚      â””â”€ Status: FROZEN (requires_grad=False)                             â”‚
â”‚                                                                            â”‚
â”‚   â””â”€ RGB Handcrafted Projection (TRAINABLE)                              â”‚
â”‚      â”œâ”€ Input: (B, 84)                                                    â”‚
â”‚      â”œâ”€ Layer 1: Linear(84 â†’ 128) + BatchNorm + ReLU + Dropout(0.3)    â”‚
â”‚      â”œâ”€ Layer 2: Linear(128 â†’ 128)                                       â”‚
â”‚      â””â”€ Output: (B, 128)                                                  â”‚
â”‚                                                                            â”‚
â”‚   â””â”€ RGB Fusion Layer                                                     â”‚
â”‚      â”œâ”€ Concat: CNN features (512) + Handcrafted (128) = 640            â”‚
â”‚      â”œâ”€ Linear(640 â†’ 512)                                                 â”‚
â”‚      â””â”€ Output: (B, 512, 7, 7) [reshaped]                                â”‚
â”‚                                                                            â”‚
â”‚  BRANCH 2: MULTISPECTRAL ENCODER (TRAINABLE)                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚   â””â”€ MS Encoder (5 â†’ 128 channels)                                       â”‚
â”‚      â”œâ”€ Conv2d(5â†’32, k=3, p=1) + BN + ReLU + MaxPool(2)                 â”‚
â”‚      â”œâ”€ Conv2d(32â†’64, k=3, p=1) + BN + ReLU + MaxPool(2)                â”‚
â”‚      â”œâ”€ Conv2d(64â†’128, k=3, p=1) + BN + ReLU + AdaptiveAvgPool(7Ã—7)    â”‚
â”‚      â””â”€ Output: (B, 128, 7, 7)                                           â”‚
â”‚                                                                            â”‚
â”‚  BRANCH 3: HYPERSPECTRAL ENCODER (TRAINABLE)                              â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚   â””â”€ HS Encoder (125 â†’ 128 channels)                                     â”‚
â”‚      â”œâ”€ Conv2d(125â†’64, k=3, p=1) + BN + ReLU + MaxPool(2)               â”‚
â”‚      â”œâ”€ Conv2d(64â†’128, k=3, p=1) + BN + ReLU + AdaptiveAvgPool(7Ã—7)    â”‚
â”‚      â””â”€ Output: (B, 128, 7, 7)                                           â”‚
â”‚                                                                            â”‚
â”‚  SPECTRAL FUSION (MS + HS)                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚   â””â”€ MultiHeadSpectralAttention (TRAINABLE)                              â”‚
â”‚      â”œâ”€ Input: MS (B,128,7,7) + HS (B,128,7,7)                          â”‚
â”‚      â”œâ”€ Mechanism: Multi-head Attention (4 heads)                        â”‚
â”‚      â”œâ”€ Dropout: 0.4                                                      â”‚
â”‚      â””â”€ Output: (B, 256, 7, 7)                                           â”‚
â”‚                                                                            â”‚
â”‚   â””â”€ Spectral Projection                                                  â”‚
â”‚      â”œâ”€ Conv2d(256â†’512, k=1) + BN + ReLU + Dropout2d(0.4)               â”‚
â”‚      â””â”€ Output: (B, 512, 7, 7)                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DECISION LAYERS (TWO-STEP)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  STEP 1 DECISION PATH: Healthy vs Diseased                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚   â”œâ”€ BiasedContextGate (RGB Weight: 85%)                                 â”‚
â”‚   â”‚  â”œâ”€ Input: F_RGB (B,512,7,7) + F_Spectral (B,512,7,7)               â”‚
â”‚   â”‚  â”œâ”€ Gate Network:                                                     â”‚
â”‚   â”‚  â”‚  â””â”€ Conv2d(1024â†’512, k=1) + BN + ReLU                            â”‚
â”‚   â”‚  â”‚  â””â”€ Conv2d(512â†’1, k=1) + Sigmoid                                  â”‚
â”‚   â”‚  â”œâ”€ Fusion: 0.85Ã—F_RGB + 0.15Ã—GateÃ—F_Spectral                       â”‚
â”‚   â”‚  â””â”€ Output: (B, 512, 7, 7)                                           â”‚
â”‚   â”‚                                                                        â”‚
â”‚   â””â”€ Classification Head                                                  â”‚
â”‚      â”œâ”€ AdaptiveAvgPool2d(1Ã—1) â†’ Flatten                                 â”‚
â”‚      â”œâ”€ Linear(512â†’256) + BN + ReLU + Dropout(0.5)                       â”‚
â”‚      â”œâ”€ Linear(256â†’128) + BN + ReLU + Dropout(0.5)                       â”‚
â”‚      â”œâ”€ Linear(128â†’2)                                                     â”‚
â”‚      â””â”€ Output: (B, 2) [Healthy=0, Diseased=1]                           â”‚
â”‚                                                                            â”‚
â”‚  STEP 2 DECISION PATH: Rust vs Other                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚   â”œâ”€ BiasedContextGate (RGB Weight: 30%)                                 â”‚
â”‚   â”‚  â”œâ”€ Input: F_RGB (B,512,7,7) + F_Spectral (B,512,7,7)               â”‚
â”‚   â”‚  â”œâ”€ Gate Network: [Same structure as Step 1]                         â”‚
â”‚   â”‚  â”œâ”€ Fusion: 0.30Ã—F_RGB + 0.70Ã—GateÃ—F_Spectral                       â”‚
â”‚   â”‚  â””â”€ Output: (B, 512, 7, 7)                                           â”‚
â”‚   â”‚                                                                        â”‚
â”‚   â””â”€ Classification Head                                                  â”‚
â”‚      â”œâ”€ AdaptiveAvgPool2d(1Ã—1) â†’ Flatten                                 â”‚
â”‚      â”œâ”€ Linear(512â†’256) + BN + ReLU + Dropout(0.5)                       â”‚
â”‚      â”œâ”€ Linear(256â†’128) + BN + ReLU + Dropout(0.5)                       â”‚
â”‚      â”œâ”€ Linear(128â†’2)                                                     â”‚
â”‚      â””â”€ Output: (B, 2) [Rust=0, Other=1]                                 â”‚
â”‚                                                                            â”‚
â”‚  FINAL 3-CLASS OUTPUT (Probabilistic Combination)                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚   Step1_probs = Softmax(Step1_logits)  # [P(H), P(D)]                   â”‚
â”‚   Step2_probs = Softmax(Step2_logits)  # [P(R|D), P(O|D)]               â”‚
â”‚                                                                            â”‚
â”‚   P(Healthy) = Step1_probs[0]                                            â”‚
â”‚   P(Rust)    = Step1_probs[1] Ã— Step2_probs[0]                          â”‚
â”‚   P(Other)   = Step1_probs[1] Ã— Step2_probs[1]                          â”‚
â”‚                                                                            â”‚
â”‚   Final_logits = log([P(H), P(R), P(O)])                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
III. QUY TRÃŒNH HUáº¤N LUYá»†N (3 PHASES)
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: TRAIN STEP 1 (Healthy vs Diseased)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  Má»¥c tiÃªu: PhÃ¢n biá»‡t lÃ¡ khá»e máº¡nh vs lÃ¡ bá»‡nh                             â”‚
â”‚                                                                            â”‚
â”‚  Cáº¥u hÃ¬nh Training:                                                        â”‚
â”‚   â”œâ”€ Epochs: 20                                                           â”‚
â”‚   â”œâ”€ Optimizer: AdamW(lr=1e-4, weight_decay=1e-3)                        â”‚
â”‚   â”œâ”€ Scheduler: ReduceLROnPlateau(patience=5, factor=0.5)               â”‚
â”‚   â”œâ”€ Loss: CrossEntropyLoss vá»›i class weights [1.5, 1.0]                â”‚
â”‚   â”‚   â””â”€ Boost Healthy class Ä‘á»ƒ giáº£m false negatives                     â”‚
â”‚   â”œâ”€ RGB Weight: 85% (Spectral: 15%)                                     â”‚
â”‚   â””â”€ RGB Handcrafted Features: ENABLED                                    â”‚
â”‚                                                                            â”‚
â”‚  Trainable Components:                                                     â”‚
â”‚   âœ“ MS Encoder (5â†’128 channels)                                          â”‚
â”‚   âœ“ HS Encoder (125â†’128 channels)                                        â”‚
â”‚   âœ“ Spectral Attention Fusion                                             â”‚
â”‚   âœ“ Spectral Projection                                                   â”‚
â”‚   âœ“ RGB Handcrafted Projection                                            â”‚
â”‚   âœ“ RGB Fusion Layer                                                      â”‚
â”‚   âœ“ Step 1 BiasedContextGate                                              â”‚
â”‚   âœ“ Step 1 Classification Head                                            â”‚
â”‚   âœ— RGB Backbone (ResNet18) - FROZEN                                     â”‚
â”‚                                                                            â”‚
â”‚  Data Augmentation:                                                        â”‚
â”‚   RGB:                                                                     â”‚
â”‚    â”œâ”€ Resize(256) â†’ CenterCrop(224)                                      â”‚
â”‚    â”œâ”€ HorizontalFlip(p=0.5)                                              â”‚
â”‚    â”œâ”€ VerticalFlip(p=0.5)                                                â”‚
â”‚    â”œâ”€ Rotate(limit=15Â°, p=0.5)                                           â”‚
â”‚    â””â”€ RandomBrightnessContrast(p=0.3)                                    â”‚
â”‚                                                                            â”‚
â”‚   MS/HS:                                                                   â”‚
â”‚    â”œâ”€ Resize(256) â†’ CenterCrop(224)                                      â”‚
â”‚    â”œâ”€ HorizontalFlip(p=0.5)                                              â”‚
â”‚    â”œâ”€ VerticalFlip(p=0.5)                                                â”‚
â”‚    â”œâ”€ Rotate(limit=15Â°, p=0.5)                                           â”‚
â”‚    â”œâ”€ Gaussian Noise (Ïƒ=0.01, p=0.3)                                     â”‚
â”‚    â””â”€ Random Channel Dropout (p=0.2)                                     â”‚
â”‚                                                                            â”‚
â”‚  Output:                                                                   â”‚
â”‚   â””â”€ Checkpoint: checkpoints/best_model_step1.pth                         â”‚
â”‚   â””â”€ Best Val Accuracy lÆ°u theo Step 1 binary task                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: TRAIN STEP 2 (Rust vs Other)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  Má»¥c tiÃªu: PhÃ¢n biá»‡t bá»‡nh gá»‰ sáº¯t vs cÃ¡c bá»‡nh khÃ¡c                        â”‚
â”‚                                                                            â”‚
â”‚  Cáº¥u hÃ¬nh Training:                                                        â”‚
â”‚   â”œâ”€ Epochs: 20                                                           â”‚
â”‚   â”œâ”€ Optimizer: AdamW(lr=1e-4, weight_decay=1e-3)                        â”‚
â”‚   â”œâ”€ Scheduler: ReduceLROnPlateau(patience=5, factor=0.5)               â”‚
â”‚   â”œâ”€ Loss: CrossEntropyLoss vá»›i class weights [1.0, 1.0]                â”‚
â”‚   â”œâ”€ RGB Weight: 30% (Spectral: 70%)                                     â”‚
â”‚   â”œâ”€ RGB Handcrafted Features: DISABLED                                   â”‚
â”‚   â””â”€ Training Data: CHá»ˆ máº«u Diseased (label > 0)                         â”‚
â”‚                                                                            â”‚
â”‚  Frozen Components (tá»« Phase 1):                                          â”‚
â”‚   âœ— Step 1 BiasedContextGate                                              â”‚
â”‚   âœ— Step 1 Classification Head                                            â”‚
â”‚                                                                            â”‚
â”‚  Trainable Components:                                                     â”‚
â”‚   âœ“ MS Encoder (fine-tune)                                               â”‚
â”‚   âœ“ HS Encoder (fine-tune)                                               â”‚
â”‚   âœ“ Spectral Attention Fusion (fine-tune)                                â”‚
â”‚   âœ“ Spectral Projection (fine-tune)                                      â”‚
â”‚   âœ“ Step 2 BiasedContextGate                                              â”‚
â”‚   âœ“ Step 2 Classification Head                                            â”‚
â”‚                                                                            â”‚
â”‚  Data Filtering:                                                           â”‚
â”‚   Chá»‰ sá»­ dá»¥ng samples cÃ³ label âˆˆ {1, 2} (Rust, Other)                   â”‚
â”‚   Chuyá»ƒn labels: 1â†’0 (Rust), 2â†’1 (Other)                                 â”‚
â”‚                                                                            â”‚
â”‚  Output:                                                                   â”‚
â”‚   â””â”€ Checkpoint: checkpoints/best_model_step2.pth                         â”‚
â”‚   â””â”€ Best Val Accuracy lÆ°u theo Step 2 binary task                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: FINAL EVALUATION (3-Class)                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  Load Model: checkpoints/best_model_step2.pth                             â”‚
â”‚                                                                            â”‚
â”‚  Inference Mode:                                                           â”‚
â”‚   â”œâ”€ Forward pass vá»›i step='both'                                        â”‚
â”‚   â”œâ”€ Káº¿t há»£p xÃ¡c suáº¥t tá»« 2 bÆ°á»›c                                          â”‚
â”‚   â””â”€ ÄÃ¡nh giÃ¡ trÃªn toÃ n bá»™ 3 classes                                      â”‚
â”‚                                                                            â”‚
â”‚  Evaluation Metrics:                                                       â”‚
â”‚   â”œâ”€ Overall Accuracy                                                     â”‚
â”‚   â”œâ”€ Per-class Precision/Recall/F1-score                                 â”‚
â”‚   â””â”€ Confusion Matrix (3Ã—3)                                               â”‚
â”‚                                                                            â”‚
â”‚  Output:                                                                   â”‚
â”‚   â””â”€ Checkpoint: checkpoints/best_model_twostep_final.pth                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
IV. CHI TIáº¾T CÃC MODULE CHÃNH
================================================================================

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. BiasedContextGate Module
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Má»¥c Ä‘Ã­ch: Äiá»u chá»‰nh tá»· trá»ng giá»¯a RGB vÃ  Spectral features

Architecture:
    Input: F_RGB (B,C,H,W), F_Spectral (B,C,H,W)
    
    Gate Network:
        Concat [F_RGB, F_Spectral] â†’ (B, 2C, H, W)
        â†“
        Conv2d(2C â†’ C, k=1) + BatchNorm + ReLU
        â†“
        Conv2d(C â†’ 1, k=1) + Sigmoid â†’ Gate (B, 1, H, W)
    
    Biased Fusion:
        Output = rgb_weight Ã— F_RGB + spec_weight Ã— Gate Ã— F_Spectral
    
    Hyperparameters:
        Step 1: rgb_weight=0.85, spec_weight=0.15
        Step 2: rgb_weight=0.30, spec_weight=0.70

Giáº£i thÃ­ch:
    - Gate há»c cÃ¡ch chá»n lá»c thÃ´ng tin Spectral phÃ¹ há»£p
    - rgb_weight/spec_weight Ä‘iá»u chá»‰nh bias cá»‘ Ä‘á»‹nh
    - Step 1 Æ°u tiÃªn RGB (mÃ u sáº¯c) cho Healthy detection
    - Step 2 Æ°u tiÃªn Spectral (phá»•) cho disease discrimination


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2. MultiHeadSpectralAttention Module
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Má»¥c Ä‘Ã­ch: Fusion MS vÃ  HS thÃ´ng qua cross-attention

Architecture:
    Input: MS (B,128,7,7), HS (B,128,7,7)
    
    Reshape to sequence:
        MS â†’ (B, 49, 128)  # 7Ã—7 spatial positions
        HS â†’ (B, 49, 128)
    
    Multi-head Attention:
        Concat [MS, HS] â†’ (B, 49, 256)
        â†“
        MultiheadAttention(embed_dim=256, num_heads=4, dropout=0.1)
        â†“
        Output (B, 49, 256)
    
    Reshape back:
        (B, 49, 256) â†’ (B, 256, 7, 7)

Giáº£i thÃ­ch:
    - Há»c correlation giá»¯a MS vÃ  HS á»Ÿ má»—i vá»‹ trÃ­ khÃ´ng gian
    - 4 heads cho phÃ©p há»c nhiá»u patterns khÃ¡c nhau
    - Dropout 0.1 Ä‘á»ƒ regularization


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
3. RGB Handcrafted Feature Extraction
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Feature Engineering Pipeline:

    A. GLCM Texture Features (36 dims)
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       Input: RGB image
       
       For each channel (R, G, B):
           â”œâ”€ Compute GLCM matrix
           â”‚  â””â”€ distances=[1], angles=[0Â°, 45Â°, 90Â°, 135Â°]
           â”œâ”€ Extract properties:
           â”‚  â”œâ”€ Contrast (4 angles Ã— 1 distance = 4 values)
           â”‚  â”œâ”€ Homogeneity (4 values)
           â”‚  â””â”€ Correlation (4 values)
           â””â”€ Total: 12 values per channel
       
       Output: 12 Ã— 3 channels = 36 features

    B. LBP Multiscale Features (30 dims)
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       Input: Green channel (most informative for vegetation)
       
       For each radius R âˆˆ {1, 2, 3}:
           â”œâ”€ Compute LBP with P=8 neighbors
           â”œâ”€ Use 'uniform' pattern (10 bins)
           â””â”€ Normalize histogram
       
       Output: 10 Ã— 3 scales = 30 features

    C. Color Features (18 dims)
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       C1. HSV Statistics (12 dims):
           For each channel (H, S, V):
               â””â”€ [mean, std, min, max] = 4 Ã— 3 = 12
       
       C2. ExG (Excess Green Index) (3 dims):
           ExG = 2Ã—G - R - B
           â””â”€ [mean, std, range] = 3
       
       C3. VARI (Visible Atmospherically Resistant Index) (3 dims):
           VARI = (G - R) / (G + R - B)
           â””â”€ [mean, std, range] = 3
       
       Output: 12 + 3 + 3 = 18 features

Total RGB Handcrafted Features: 36 + 30 + 18 = 84 dimensions

Usage:
    - Chá»‰ sá»­ dá»¥ng trong Step 1 (Healthy vs Diseased)
    - KhÃ´ng dÃ¹ng trong Step 2 (vÃ¬ Spectral Ä‘Ã£ Ä‘á»§ máº¡nh)
    - Projection: 84 â†’ 128 â†’ Concat vá»›i CNN features


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4. Data Augmentation Strategy
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Philosophy: Light augmentation Ä‘á»ƒ trÃ¡nh overfitting nhÆ°ng giá»¯ tÃ­nh cháº¥t váº­t lÃ½

RGB Augmentation (via Albumentations):
    Preprocessing:
        Resize(256Ã—256)
    
    Geometric (if training):
        â”œâ”€ HorizontalFlip(p=0.5)
        â”œâ”€ VerticalFlip(p=0.5)
        â””â”€ Rotate(limit=Â±15Â°, p=0.5)
    
    Photometric (if training):
        â””â”€ RandomBrightnessContrast(
               brightness_limit=0.15,
               contrast_limit=0.15,
               p=0.3
           )
    
    Finalization:
        â”œâ”€ CenterCrop(224Ã—224)
        â””â”€ Normalize(mean=ImageNet, std=ImageNet)

MS/HS Augmentation:
    Preprocessing:
        Resize(256Ã—256)
    
    Geometric (same as RGB):
        â”œâ”€ HorizontalFlip(p=0.5)
        â”œâ”€ VerticalFlip(p=0.5)
        â””â”€ Rotate(limit=Â±15Â°, p=0.5)
    
    Finalization:
        CenterCrop(224Ã—224)
    
    Custom Spectral Augmentation (if training):
        â”œâ”€ Gaussian Noise (p=0.3):
        â”‚  â””â”€ Add noise ~ N(0, 0.01)
        â”‚
        â””â”€ Channel Dropout (p=0.2):
           â”œâ”€ MS: Drop 1 random band
           â””â”€ HS: Drop 1-2 random bands
    
    Clipping:
        All values clamped to [0, 1]

Note:
    - Augmentation parameters KHÃC NHAU cho RGB vs MS/HS
    - TrÃ¡nh normalize MS/HS quÃ¡ máº¡nh (giá»¯ physical meaning)
    - Random seed KHÃ”NG sync giá»¯a modalities (tÄƒng diversity)


================================================================================
V. THÃ”NG Sá» MÃ” HÃŒNH
================================================================================

Total Parameters Breakdown:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. RGB Branch:
   â”œâ”€ ResNet18 Backbone: ~11.2M params (FROZEN)
   â”œâ”€ Handcrafted Projection: ~11K params (84â†’128â†’128)
   â””â”€ RGB Fusion Layer: ~328K params (640â†’512)

2. MS Encoder:
   â””â”€ Total: ~103K params (5â†’32â†’64â†’128)

3. HS Encoder:
   â””â”€ Total: ~147K params (125â†’64â†’128)

4. Spectral Fusion:
   â”œâ”€ MultiHeadAttention: ~262K params (256-dim, 4 heads)
   â””â”€ Projection: ~131K params (256â†’512)

5. Step 1 Components:
   â”œâ”€ BiasedContextGate: ~263K params
   â””â”€ Classification Head: ~229K params (512â†’256â†’128â†’2)

6. Step 2 Components:
   â”œâ”€ BiasedContextGate: ~263K params
   â””â”€ Classification Head: ~229K params (512â†’256â†’128â†’2)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~13.2M parameters
Trainable: ~2.0M parameters (~15%)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


Training Configuration:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Hyperparameters:
    Batch Size: [TÃ¹y GPU memory, thÆ°á»ng 8-16]
    Learning Rate: 1e-4
    Weight Decay: 1e-3
    Optimizer: AdamW
    Scheduler: ReduceLROnPlateau(mode='max', patience=5, factor=0.5)
    
    Dropout:
        â”œâ”€ Feature extraction: 0.4
        â”œâ”€ Classifier hidden layers: 0.5
        â””â”€ Handcrafted projection: 0.3

Class Weights:
    Phase 1 (Step 1):
        [Healthy: 1.5, Diseased: 1.0]
        â””â”€ Boost Healthy Ä‘á»ƒ giáº£m false negatives
    
    Phase 2 (Step 2):
        [Rust: 1.0, Other: 1.0]
        â””â”€ Balanced (hoáº·c Ä‘iá»u chá»‰nh theo tá»· lá»‡ thá»±c táº¿)

Regularization Techniques:
    âœ“ Weight Decay (L2 regularization)
    âœ“ Dropout layers
    âœ“ Batch Normalization
    âœ“ Data Augmentation
    âœ“ Early Stopping (implicit via best model saving)


================================================================================
VI. CHIáº¾N LÆ¯á»¢C INFERENCE
================================================================================

Forward Pass Logic:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Input: RGB, MS, HS, [RGB_handcrafted]

Step 1: Extract Shared Features
    â”œâ”€ F_RGB_CNN = ResNet18(RGB)  # (B, 512, 7, 7)
    â”œâ”€ F_MS = MS_Encoder(MS)      # (B, 128, 7, 7)
    â”œâ”€ F_HS = HS_Encoder(HS)      # (B, 128, 7, 7)
    â””â”€ F_Spectral = Fusion(F_MS, F_HS)  # (B, 512, 7, 7)

Step 2: Enhance RGB (if Step 1)
    â”œâ”€ F_RGB_pooled = AdaptiveAvgPool(F_RGB_CNN)  # (B, 512)
    â”œâ”€ F_handcrafted = Projection(RGB_handcrafted)  # (B, 128)
    â”œâ”€ F_concat = Concat[F_RGB_pooled, F_handcrafted]  # (B, 640)
    â”œâ”€ F_RGB_fused = Linear(F_concat)  # (B, 512)
    â””â”€ F_RGB = Reshape(F_RGB_fused, (B, 512, 7, 7))

Step 3: Biased Fusion & Classification
    Mode 'step1':
        â”œâ”€ F_step1 = BiasedGate1(F_RGB, F_Spectral)  # 85% RGB
        â””â”€ Output = Head1(F_step1)  # (B, 2)
    
    Mode 'step2':
        â”œâ”€ F_step2 = BiasedGate2(F_RGB, F_Spectral)  # 30% RGB
        â””â”€ Output = Head2(F_step2)  # (B, 2)
    
    Mode 'both':
        â”œâ”€ Step1_logits = Head1(BiasedGate1(...))
        â”œâ”€ Step2_logits = Head2(BiasedGate2(...))
        â”‚
        â”œâ”€ P1 = Softmax(Step1_logits)  # [P(H), P(D)]
        â”œâ”€ P2 = Softmax(Step2_logits)  # [P(R|D), P(O|D)]
        â”‚
        â”œâ”€ P(Healthy) = P1[0]
        â”œâ”€ P(Rust) = P1[1] Ã— P2[0]
        â”œâ”€ P(Other) = P1[1] Ã— P2[1]
        â”‚
        â””â”€ Final_logits = log([P(H), P(R), P(O)])


Prediction Pipeline:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Load Model:
   model = TwoStepWheatNet()
   model.load_state_dict(torch.load('best_model_twostep_final.pth'))
   model.eval()

2. Preprocess Input:
   â”œâ”€ RGB: Resize â†’ CenterCrop â†’ Normalize
   â”œâ”€ MS: Resize â†’ CenterCrop â†’ [0,1]
   â”œâ”€ HS: Resize â†’ CenterCrop â†’ [0,1]
   â””â”€ Handcrafted: Extract features tá»« RGB gá»‘c

3. Forward Pass:
   logits = model(rgb, ms, hs, rgb_handcrafted=None, step='both')
   probs = F.softmax(logits, dim=1)
   pred = torch.argmax(probs, dim=1)

4. Post-processing:
   Class Mapping:
       0 â†’ 'Healthy'
       1 â†’ 'Rust'
       2 â†’ 'Other'


================================================================================
VII. ÄÃNH GIÃ VÃ€ METRICS
================================================================================

Evaluation Metrics:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Per-Phase Metrics:
   
   Phase 1 (Step 1: Binary Healthy vs Diseased):
       â”œâ”€ Accuracy
       â”œâ”€ Precision, Recall, F1-score (per class)
       â””â”€ Confusion Matrix (2Ã—2):
              Predicted: Healthy | Diseased
          True Healthy:   TP_H   |  FN_H
          True Diseased:  FP_H   |  TN_H

   Phase 2 (Step 2: Binary Rust vs Other):
       â”œâ”€ Accuracy (on diseased samples only)
       â”œâ”€ Precision, Recall, F1-score (per class)
       â””â”€ Confusion Matrix (2Ã—2):
              Predicted: Rust | Other
          True Rust:     TP_R |  FN_R
          True Other:    FP_R |  TN_R

   Phase 3 (Final 3-Class):
       â”œâ”€ Overall Accuracy
       â”œâ”€ Per-class Precision, Recall, F1-score
       â””â”€ Confusion Matrix (3Ã—3):
              Predicted: Health | Rust | Other
          True Health:    TP_H  | FN_HR | FN_HO
          True Rust:      FP_RH | TP_R  | FN_RO
          True Other:     FP_OH | FP_OR | TP_O


2. Expected Performance:
   
   Best Case Scenario (vá»›i dá»¯ liá»‡u tá»‘t):
       Step 1 Acc: > 90% (Healthy vs Diseased rÃµ rÃ ng)
       Step 2 Acc: > 85% (Rust vs Other khÃ³ hÆ¡n)
       Final 3-Class Acc: > 80%
   
   Critical Metrics to Monitor:
       â”œâ”€ Healthy Recall (trÃ¡nh miss bá»‡nh)
       â”œâ”€ Rust Precision (trÃ¡nh false alarm cho bá»‡nh nghiÃªm trá»ng)
       â””â”€ Overall Balanced Accuracy (trÃ¡nh bias vÃ o class Ä‘a sá»‘)


3. Confusion Analysis:
   
   Common Errors Expected:
       â”œâ”€ Health â†” Other (Ã­t, nhá» Step 1 máº¡nh)
       â”œâ”€ Health â†” Rust (ráº¥t Ã­t, Ä‘Ã£ giáº£i quyáº¿t báº±ng two-step)
       â””â”€ Rust â†” Other (cÃ³ thá»ƒ cao náº¿u Spectral signal yáº¿u)
   
   Debugging Strategy:
       Náº¿u Step 1 Acc tháº¥p â†’ Kiá»ƒm tra RGB features/augmentation
       Náº¿u Step 2 Acc tháº¥p â†’ Kiá»ƒm tra Spectral features/SNR
       Náº¿u Final Acc tháº¥p nhÆ°ng Steps tá»‘t â†’ Kiá»ƒm tra probability fusion logic


================================================================================
VIII. Cáº¤U TRÃšC FILE VÃ€ DEPENDENCIES
================================================================================

File Organization:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Project Root: d:\Kaggle\QuangPho\

â”œâ”€ src/
â”‚  â”œâ”€ model/
â”‚  â”‚  â”œâ”€ Layer1/
â”‚  â”‚  â”‚  â”œâ”€ CNNRGB.py               # ResNet18 RGB extractor
â”‚  â”‚  â”‚  â”œâ”€ HSExaction.py           # HS feature extractor
â”‚  â”‚  â”‚  â”œâ”€ MSExaction.py           # MS feature extractor
â”‚  â”‚  â”‚  â””â”€ RGBExaction.py          # RGB dataset loader
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ Layer2/
â”‚  â”‚  â”‚  â”œâ”€ AttentionFusion.py      # MultiHeadSpectralAttention
â”‚  â”‚  â”‚  â”œâ”€ FullFusion.py           # Alternative fusion methods
â”‚  â”‚  â”‚  â””â”€ SoftCompetitionFusion.py
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ OutputLayer/
â”‚  â”‚  â”‚  â””â”€ OutputLayer.py          # Classification heads
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ Pipeline/
â”‚  â”‚     â”œâ”€ PipelineV1_Stage1_TwoStep.py  # â˜… MAIN MODEL
â”‚  â”‚     â”œâ”€ PipelineV1_Stage1.py
â”‚  â”‚     â”œâ”€ PipelineV1_Improved.py
â”‚  â”‚     â”œâ”€ PipelineV1_KFold.py
â”‚  â”‚     â””â”€ PipelineV1.py
â”‚  â”‚
â”‚  â”œâ”€ FeatureEngineering/
â”‚  â”‚  â”œâ”€ RGBFeature.py              # GLCM, LBP, Color indices
â”‚  â”‚  â”œâ”€ MSFeature.py               # MS spectral indices
â”‚  â”‚  â””â”€ HSFeature.py               # HS spectral signatures
â”‚  â”‚
â”‚  â””â”€ Preprocessing/
â”‚     â”œâ”€ Preprocessing.py           # General preprocessing
â”‚     â””â”€ PreRGB.py                  # RGB transforms
â”‚
â”œâ”€ Data/
â”‚  â”œâ”€ train/
â”‚  â”‚  â”œâ”€ RGB/    # Health*.png, Rust*.png, Other*.png
â”‚  â”‚  â”œâ”€ MS/     # *.tif (5 bands)
â”‚  â”‚  â””â”€ HS/     # *.tif (125 bands)
â”‚  â”‚
â”‚  â””â”€ val/
â”‚     â”œâ”€ RGB/
â”‚     â”œâ”€ MS/
â”‚     â””â”€ HS/
â”‚
â””â”€ checkpoints/
   â”œâ”€ best_model_step1.pth          # Phase 1 output
   â”œâ”€ best_model_step2.pth          # Phase 2 output
   â””â”€ best_model_twostep_final.pth  # Final model


Dependencies:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Core Libraries:
    â”œâ”€ torch >= 1.12
    â”œâ”€ torchvision >= 0.13
    â”œâ”€ numpy >= 1.21
    â”œâ”€ opencv-python >= 4.6
    â”œâ”€ Pillow >= 9.0
    â””â”€ scikit-image >= 0.19

Data Handling:
    â”œâ”€ tifffile >= 2022.5.4  (for MS/HS .tif reading)
    â”œâ”€ albumentations >= 1.3  (augmentation pipeline)
    â””â”€ scikit-learn >= 1.0  (metrics, class_weight)

Optional:
    â”œâ”€ matplotlib (visualization)
    â”œâ”€ seaborn (confusion matrix plotting)
    â””â”€ tensorboard (training monitoring)


Installation:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install albumentations tifffile opencv-python Pillow scikit-image scikit-learn


================================================================================
IX. HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG
================================================================================

1. Chuáº©n bá»‹ Dá»¯ liá»‡u:
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Cáº¥u trÃºc thÆ° má»¥c:
       Data/
       â”œâ”€ train/
       â”‚  â”œâ”€ RGB/
       â”‚  â”‚  â”œâ”€ Health_001.png
       â”‚  â”‚  â”œâ”€ Rust_001.png
       â”‚  â”‚  â””â”€ Other_001.png
       â”‚  â”œâ”€ MS/
       â”‚  â”‚  â”œâ”€ Health_001.tif  (5 bands)
       â”‚  â”‚  â”œâ”€ Rust_001.tif
       â”‚  â”‚  â””â”€ Other_001.tif
       â”‚  â””â”€ HS/
       â”‚     â”œâ”€ Health_001.tif  (125 bands)
       â”‚     â”œâ”€ Rust_001.tif
       â”‚     â””â”€ Other_001.tif
       â””â”€ val/ (cáº¥u trÃºc tÆ°Æ¡ng tá»±)
   
   Naming Convention:
       - Báº¯t Ä‘áº§u báº±ng class name: Health*, Rust*, Other*
       - CÃ¹ng stem name cho 3 modalities (RGB, MS, HS)


2. Training:
   â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   python src/model/Pipeline/PipelineV1_Stage1_TwoStep.py
   
   Quy trÃ¬nh tá»± Ä‘á»™ng:
       Phase 1: Train Step 1 (20 epochs)
       â†’ Save best_model_step1.pth
       
       Phase 2: Train Step 2 (20 epochs)
       â†’ Save best_model_step2.pth
       
       Phase 3: Evaluate 3-class
       â†’ Save best_model_twostep_final.pth


3. Inference:
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   import torch
   from src.model.Pipeline.PipelineV1_Stage1_TwoStep import TwoStepWheatNet
   
   # Load model
   model = TwoStepWheatNet()
   model.load_state_dict(torch.load('checkpoints/best_model_twostep_final.pth'))
   model.eval()
   
   # Prepare input (example)
   rgb = preprocess_rgb(rgb_path)
   ms = preprocess_ms(ms_path)
   hs = preprocess_hs(hs_path)
   
   # Predict
   with torch.no_grad():
       logits = model(rgb, ms, hs, step='both')
       probs = F.softmax(logits, dim=1)
       pred = torch.argmax(probs, dim=1)
   
   class_names = ['Healthy', 'Rust', 'Other']
   print(f"Prediction: {class_names[pred.item()]}")
   print(f"Confidence: {probs[0, pred].item():.2%}")


4. Fine-tuning:
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Äá»ƒ fine-tune cho dataset má»›i:
       1. Load checkpoint:
          model.load_state_dict(torch.load('checkpoints/best_model_twostep_final.pth'))
       
       2. Unfreeze components cáº§n train:
          # VÃ­ dá»¥: Chá»‰ fine-tune classification heads
          for param in model.step1_head.parameters():
              param.requires_grad = True
          for param in model.step2_head.parameters():
              param.requires_grad = True
       
       3. Train vá»›i learning rate tháº¥p hÆ¡n:
          optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()),
                           lr=1e-5)


================================================================================
X. Káº¾T LUáº¬N VÃ€ HÆ¯á»šNG PHÃT TRIá»‚N
================================================================================

Äiá»ƒm Máº¡nh cá»§a MÃ´ hÃ¬nh:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âœ“ Hierarchical Decision Making: Giáº£i quyáº¿t váº¥n Ä‘á» confusion giá»¯a Health-Rust
âœ“ Multi-modal Fusion: Táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a RGB, MS, HS
âœ“ Adaptive Weighting: BiasedContextGate Ä‘iá»u chá»‰nh tá»· trá»ng theo task
âœ“ Feature Engineering: Káº¿t há»£p Deep Learning + Handcrafted features
âœ“ Robust Training: Regularization máº¡nh máº½, data augmentation Ä‘a dáº¡ng


Giá»›i Háº¡n vÃ  ThÃ¡ch Thá»©c:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

âš  Data Hungry: Cáº§n lÆ°á»£ng lá»›n labeled data cho 3 modalities
âš  Computational Cost: Inference cháº­m hÆ¡n single-step models
âš  Error Propagation: Lá»—i Step 1 áº£nh hÆ°á»Ÿng Ä‘áº¿n Step 2
âš  Calibration: XÃ¡c suáº¥t cáº§n calibrate ká»¹ cho final fusion
âš  Generalization: CÃ³ thá»ƒ overfit náº¿u data khÃ´ng Ä‘á»§ diverse


HÆ°á»›ng PhÃ¡t Triá»ƒn TÆ°Æ¡ng Lai:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Architecture Improvements:
   â”œâ”€ Thá»­ Transformer-based backbone (ViT, Swin)
   â”œâ”€ Dynamic gate network (há»c cáº£ spatial weights)
   â”œâ”€ Uncertainty estimation (Monte Carlo Dropout, Ensembles)
   â””â”€ Multi-scale fusion (FPN, PANet)

2. Training Enhancements:
   â”œâ”€ Curriculum Learning (easy samples â†’ hard samples)
   â”œâ”€ Contrastive Learning (Spectral signatures)
   â”œâ”€ Semi-supervised Learning (Pseudo-labeling)
   â””â”€ Domain Adaptation (Transfer across regions/seasons)

3. Data & Features:
   â”œâ”€ Active Learning Ä‘á»ƒ chá»n samples quan trá»ng
   â”œâ”€ ThÃªm Temporal features (multi-date images)
   â”œâ”€ ThÃªm Environmental metadata (weather, soil)
   â””â”€ Self-supervised pre-training trÃªn unlabeled data

4. Deployment:
   â”œâ”€ Model Quantization (INT8) cho mobile devices
   â”œâ”€ Knowledge Distillation â†’ Single-step student model
   â”œâ”€ ONNX export cho cross-platform inference
   â””â”€ TensorRT optimization cho real-time processing


Use Cases KhÃ¡c:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ adapt cho:
   â”œâ”€ Crop disease detection (corn, rice, potato, etc.)
   â”œâ”€ Plant stress monitoring (drought, nutrient deficiency)
   â”œâ”€ Food quality inspection (fruit sorting, meat grading)
   â””â”€ Remote sensing (land cover classification, change detection)


================================================================================
XI. THAM KHáº¢O VÃ€ CREDITS
================================================================================

Key Techniques Used:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Transfer Learning:
   - ResNet18 pretrained on ImageNet
   - Reference: He et al., "Deep Residual Learning" (CVPR 2016)

2. Multi-modal Fusion:
   - Attention-based fusion
   - Reference: Vaswani et al., "Attention Is All You Need" (NIPS 2017)

3. Hierarchical Classification:
   - Coarse-to-fine approach
   - Reference: Yan et al., "HD-CNN" (ICCV 2015)

4. Feature Engineering:
   - GLCM: Haralick et al. (1973)
   - LBP: Ojala et al. (2002)
   - Vegetation Indices: Tucker (1979), Gitelson et al. (2002)


Tools & Libraries:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

- PyTorch: https://pytorch.org
- Albumentations: https://albumentations.ai
- scikit-image: https://scikit-image.org
- OpenCV: https://opencv.org


================================================================================
                            --- END OF DOCUMENT ---
================================================================================

ğŸ“‹ TÃ i liá»‡u nÃ y Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng tá»« source code
ğŸ”„ Last Updated: 04/02/2026
âœ‰ï¸  Contact: [Your Email/GitHub]
ğŸ“ Repository: d:\Kaggle\QuangPho\
