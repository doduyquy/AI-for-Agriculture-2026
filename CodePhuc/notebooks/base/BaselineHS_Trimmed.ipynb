{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell_0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Bands: 101 (Indices 10 to 111)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tifffile as tiff\n",
    "import torchvision.models as models\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ===== PATHS =====\n",
    "HS_DIR = r\"D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\data\\raw\\Kaggle_Prepared\\train\\HS\"\n",
    "TEST_HS_DIR = r\"D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\data\\raw\\Kaggle_Prepared\\val\\HS\"\n",
    "CHECKPOINT_DIR = r\"D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\checkpoints\"\n",
    "CKPT_PATH = os.path.join(CHECKPOINT_DIR, \"best_hs_trimmed_resnet18.pth\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== TRIMMED DATA SETTINGS =====\n",
    "ORIGINAL_BANDS = 125\n",
    "START_BAND = 10      # Skip first 10\n",
    "END_BAND = 125 - 14  # Skip last 14 (111)\n",
    "TARGET_BANDS = END_BAND - START_BAND # 101\n",
    "TARGET_HW = (64, 64) \n",
    "\n",
    "# ===== SPLIT =====\n",
    "VAL_RATIO = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# ===== TRAIN =====\n",
    "EPOCHS =10\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "WD = 1e-4\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "print(f\"Target Bands: {TARGET_BANDS} (Indices {START_BAND} to {END_BAND})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trimmed_stats(img_dir):\n",
    "    print(\"Computing global stats for TRIMMED bands...\")\n",
    "    files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.tif', '.tiff'))])\n",
    "    \n",
    "    pixel_num = 0\n",
    "    channel_sum = np.zeros(TARGET_BANDS, dtype=np.float64)\n",
    "    channel_sum_sq = np.zeros(TARGET_BANDS, dtype=np.float64)\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        path = os.path.join(img_dir, f)\n",
    "        # Load\n",
    "        img = tiff.imread(path).astype(np.float32)\n",
    "        \n",
    "        # Strictly check for HWC format based on known Band count (125)\n",
    "        # If shape is (H, W, 125), transpose to (125, H, W)\n",
    "        if img.ndim == 3 and img.shape[-1] == ORIGINAL_BANDS:\n",
    "             img = np.transpose(img, (2, 0, 1))\n",
    "        elif img.ndim == 2:\n",
    "             img = img[None, :, :]\n",
    "        \n",
    "        # Pad if short (rare) or Trim\n",
    "        if img.shape[0] < ORIGINAL_BANDS:\n",
    "             pad = np.zeros((ORIGINAL_BANDS - img.shape[0], img.shape[1], img.shape[2]), dtype=np.float32)\n",
    "             img = np.concatenate([img, pad], axis=0)\n",
    "        \n",
    "        # === TRIMMING ===\n",
    "        # Slice [10:111]\n",
    "        img = img[START_BAND:END_BAND, :, :]\n",
    "\n",
    "        pixels = img.reshape(TARGET_BANDS, -1)\n",
    "        \n",
    "        channel_sum += pixels.sum(axis=1)\n",
    "        channel_sum_sq += (pixels ** 2).sum(axis=1)\n",
    "        pixel_num += pixels.shape[1]\n",
    "    \n",
    "    mean = channel_sum / pixel_num\n",
    "    std = np.sqrt(channel_sum_sq / pixel_num - mean ** 2)\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "def label_from_filename(fname: str) -> str:\n",
    "    return os.path.basename(fname).split(\"_\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSTrimmedDataset(Dataset):\n",
    "    def __init__(self, img_dir, file_list=None, augment=False, mean=None, std=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.augment = augment\n",
    "        self.mean = torch.tensor(mean).view(TARGET_BANDS, 1, 1).float() if mean is not None else torch.zeros(TARGET_BANDS, 1, 1)\n",
    "        self.std = torch.tensor(std).view(TARGET_BANDS, 1, 1).float() if std is not None else torch.ones(TARGET_BANDS, 1, 1)\n",
    "\n",
    "        if file_list is not None:\n",
    "            self.files = file_list\n",
    "        else:\n",
    "            self.files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(('.tif', '.tiff'))])\n",
    "        \n",
    "        labels = sorted({label_from_filename(f) for f in self.files})\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(labels)}\n",
    "        self.idx_to_class = {i: c for c, i in self.class_to_idx.items()}\n",
    "        self.y = [self.class_to_idx[label_from_filename(f)] for f in self.files]\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        path = os.path.join(self.img_dir, fname)\n",
    "        arr = tiff.imread(path).astype(np.float32)\n",
    "        \n",
    "        # Format fix: Check strict band location\n",
    "        if arr.ndim == 3 and arr.shape[-1] == ORIGINAL_BANDS:\n",
    "             arr = np.transpose(arr, (2, 0, 1))\n",
    "        elif arr.ndim == 2: \n",
    "             arr = arr[None, :, :]\n",
    "        \n",
    "        # Pad to 125 if needed (robustness)\n",
    "        if arr.shape[0] < ORIGINAL_BANDS:\n",
    "             pad = np.zeros((ORIGINAL_BANDS - arr.shape[0], arr.shape[1], arr.shape[2]), dtype=np.float32)\n",
    "             arr = np.concatenate([arr, pad], axis=0)\n",
    "            \n",
    "        # === TRIM ===\n",
    "        arr = arr[START_BAND:END_BAND, :, :]\n",
    "\n",
    "        x = torch.from_numpy(arr)\n",
    "        \n",
    "        # Resize to 64x64\n",
    "        if x.shape[1:] != TARGET_HW:\n",
    "            x = x.unsqueeze(0)\n",
    "            x = F.interpolate(x, size=TARGET_HW, mode=\"bilinear\", align_corners=False)\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Normalize\n",
    "        x = (x - self.mean) / (self.std + 1e-8)\n",
    "        \n",
    "        if self.augment:\n",
    "            if torch.rand(1) > 0.5: x = torch.flip(x, dims=[2])\n",
    "            if torch.rand(1) > 0.5: x = torch.flip(x, dims=[1])\n",
    "            k = torch.randint(0, 4, (1,)).item()\n",
    "            x = torch.rot90(x, k, dims=[1, 2])\n",
    "\n",
    "        return x, self.y[idx]\n",
    "\n",
    "class HSTrimmedTestDataset(Dataset):\n",
    "    def __init__(self, img_dir, mean=None, std=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith(('.tif', '.tiff'))])\n",
    "        self.mean = torch.tensor(mean).view(TARGET_BANDS, 1, 1).float() if mean is not None else torch.zeros(TARGET_BANDS, 1, 1)\n",
    "        self.std = torch.tensor(std).view(TARGET_BANDS, 1, 1).float() if std is not None else torch.ones(TARGET_BANDS, 1, 1)\n",
    "    \n",
    "    def __len__(self): return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        path = os.path.join(self.img_dir, fname)\n",
    "        arr = tiff.imread(path).astype(np.float32)\n",
    "        \n",
    "        if arr.ndim == 3 and arr.shape[-1] == ORIGINAL_BANDS:\n",
    "             arr = np.transpose(arr, (2, 0, 1))\n",
    "        elif arr.ndim == 2: \n",
    "             arr = arr[None, :, :]\n",
    "        \n",
    "        if arr.shape[0] < ORIGINAL_BANDS:\n",
    "             pad = np.zeros((ORIGINAL_BANDS - arr.shape[0], arr.shape[1], arr.shape[2]), dtype=np.float32)\n",
    "             arr = np.concatenate([arr, pad], axis=0)\n",
    "             \n",
    "        arr = arr[START_BAND:END_BAND, :, :]\n",
    "        x = torch.from_numpy(arr)\n",
    "        \n",
    "        if x.shape[1:] != TARGET_HW:\n",
    "            x = x.unsqueeze(0)\n",
    "            x = F.interpolate(x, size=TARGET_HW, mode=\"bilinear\", align_corners=False)\n",
    "            x = x.squeeze(0)\n",
    "            \n",
    "        x = (x - self.mean) / (self.std + 1e-8)\n",
    "        return x, fname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell_3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing global stats for TRIMMED bands...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:01<00:00, 453.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed Stats Shape: (101,)\n",
      "Train samples: 480\n",
      "Batch Shape: torch.Size([32, 101, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "mean_stats, std_stats = compute_trimmed_stats(HS_DIR)\n",
    "print(\"Trimmed Stats Shape:\", mean_stats.shape)\n",
    "\n",
    "all_files = sorted([f for f in os.listdir(HS_DIR) if f.endswith(('.tif', '.tiff'))])\n",
    "labels = [label_from_filename(f) for f in all_files]\n",
    "indices = np.arange(len(all_files))\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    indices, test_size=VAL_RATIO, random_state=SEED, stratify=labels\n",
    ")\n",
    "\n",
    "train_files = [all_files[i] for i in train_idx]\n",
    "val_files = [all_files[i] for i in val_idx]\n",
    "\n",
    "train_ds = HSTrimmedDataset(HS_DIR, train_files, augment=True, mean=mean_stats, std=std_stats)\n",
    "val_ds   = HSTrimmedDataset(HS_DIR, val_files, augment=False, mean=mean_stats, std=std_stats)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Batch Shape:\", x.shape) # Should be (B, 101, 64, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell_4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_ds.class_to_idx)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify conv1 for 101 channels\n",
    "old_conv = model.conv1\n",
    "model.conv1 = nn.Conv2d(TARGET_BANDS, old_conv.out_channels, \n",
    "                        kernel_size=old_conv.kernel_size, stride=old_conv.stride, \n",
    "                        padding=old_conv.padding, bias=False)\n",
    "\n",
    "# Init weights smartly: take mean of RGB weights and replicate 101 times\n",
    "with torch.no_grad():\n",
    "    model.conv1.weight[:] = old_conv.weight.mean(dim=1, keepdim=True).repeat(1, TARGET_BANDS, 1, 1)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell_5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from C:\\Users\\ADMIN\\_netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphucga150625\u001b[0m (\u001b[33mphucga15062005\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\Notebooks\\00_baseline\\wandb\\run-20260130_135040-00qlqna2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/00qlqna2' target=\"_blank\">baseline_hs_trimmed</a></strong> to <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/00qlqna2' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/00qlqna2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Acc: 0.4688 | Val Acc: 0.4583\n",
      "Saved best model: 0.4583\n",
      "Epoch 2 | Train Acc: 0.5667 | Val Acc: 0.5000\n",
      "Saved best model: 0.5000\n",
      "Epoch 3 | Train Acc: 0.5958 | Val Acc: 0.5750\n",
      "Saved best model: 0.5750\n",
      "Epoch 4 | Train Acc: 0.6521 | Val Acc: 0.5167\n",
      "Epoch 5 | Train Acc: 0.6562 | Val Acc: 0.5750\n",
      "Epoch 6 | Train Acc: 0.6604 | Val Acc: 0.5333\n",
      "Epoch 7 | Train Acc: 0.6896 | Val Acc: 0.5667\n",
      "Epoch 8 | Train Acc: 0.7396 | Val Acc: 0.6000\n",
      "Saved best model: 0.6000\n",
      "Epoch 9 | Train Acc: 0.7479 | Val Acc: 0.6083\n",
      "Saved best model: 0.6083\n",
      "Epoch 10 | Train Acc: 0.7417 | Val Acc: 0.5750\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▃▄▆▆▆▇███</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▃▆▄▆▅▆██▆</td></tr><tr><td>val_loss</td><td>█▆▃█▃▂▄▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.74167</td></tr><tr><td>train_loss</td><td>0.60042</td></tr><tr><td>val_acc</td><td>0.575</td></tr><tr><td>val_loss</td><td>0.87805</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline_hs_trimmed</strong> at: <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/00qlqna2' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/00qlqna2</a><br> View project at: <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260130_135040-00qlqna2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"beyond-visible-spectrum\", name=\"baseline_hs_trimmed\")\n",
    "\n",
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss, train_acc = train_one_epoch(train_loader)\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), CKPT_PATH)\n",
    "        print(f\"Saved best model: {val_acc:.4f}\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell_6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Inference on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\checkpoints\\submission_hs_trimmed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(TEST_HS_DIR):\n",
    "    print(\"Running Inference on Test Set...\")\n",
    "    model.load_state_dict(torch.load(CKPT_PATH))\n",
    "    model.eval()\n",
    "    \n",
    "    test_ds = HSTrimmedTestDataset(TEST_HS_DIR, mean=mean_stats, std=std_stats)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    preds = []\n",
    "    ids = []\n",
    "    class_names = [train_ds.idx_to_class[i] for i in range(num_classes)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, fname in tqdm(test_loader):\n",
    "            x = x.to(device)\n",
    "            out = model(x)\n",
    "            p_idx = out.argmax(1).cpu().numpy()\n",
    "            preds.extend([class_names[i] for i in p_idx])\n",
    "            ids.extend(fname)\n",
    "            \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\"Id\": ids, \"Category\": preds})\n",
    "    csv_path = os.path.join(CHECKPOINT_DIR, \"submission_hs_trimmed.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
