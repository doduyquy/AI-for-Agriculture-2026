{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1db667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tifffile as tiff\n",
    "import torchvision.models as models\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ===== PATHS =====\n",
    "HS_DIR = r\"D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\data\\raw\\Kaggle_Prepared\\train\\HS\"\n",
    "TEST_HS_DIR = r\"D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\data\\raw\\Kaggle_Prepared\\val\\HS\"\n",
    "CHECKPOINT_DIR = r\"D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\checkpoints\"\n",
    "CKPT_PATH = os.path.join(CHECKPOINT_DIR, \"best_hs125_resnet18.pth\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== DATA SETTINGS =====\n",
    "TARGET_BANDS = 125\n",
    "TARGET_HW = (64, 64)       # Resizing to 64x64 for consistency\n",
    "\n",
    "# ===== SPLIT =====\n",
    "VAL_RATIO = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# ===== TRAIN =====\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "WD = 1e-4\n",
    "NUM_WORKERS = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c996475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS classes: ['Health', 'Other', 'Rust']\n",
      "NUM_CLASSES = 3\n"
     ]
    }
   ],
   "source": [
    "prefixes = sorted({\n",
    "    fn.split(\"_\")[0]\n",
    "    for fn in os.listdir(HS_DIR)\n",
    "    if fn.endswith(\".tif\")\n",
    "})\n",
    "\n",
    "print(\"MS classes:\", prefixes)\n",
    "print(\"NUM_CLASSES =\", len(prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "new_cell_46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compute_global_stats(img_dir, bands=125):\n",
    "    print(\"Computing global stats per band...\")\n",
    "    files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.tif', '.tiff'))])\n",
    "    \n",
    "    pixel_num = 0\n",
    "    channel_sum = np.zeros(bands, dtype=np.float64)\n",
    "    channel_sum_sq = np.zeros(bands, dtype=np.float64)\n",
    "\n",
    "    for f in tqdm(files):\n",
    "        path = os.path.join(img_dir, f)\n",
    "        # Read image (H, W, C) or (C, H, W)\n",
    "        img = tiff.imread(path).astype(np.float32)\n",
    "        \n",
    "        # Ensure (C, H, W) format for consistency\n",
    "        if img.ndim == 2:\n",
    "            img = img[None, :, :]\n",
    "        elif img.ndim == 3:\n",
    "            # Check if channels are last (H, W, C) -> transpose to (C, H, W)\n",
    "            # Heuristic: if last dim is `bands` (125 or near it) and first is not\n",
    "            if img.shape[-1] == bands or img.shape[-1] > img.shape[0]:\n",
    "                img = np.transpose(img, (2, 0, 1))\n",
    "        \n",
    "        # Clip to target bands if necessary\n",
    "        if img.shape[0] > bands:\n",
    "            img = img[:bands]\n",
    "        elif img.shape[0] < bands:\n",
    "            padding = np.zeros((bands - img.shape[0], img.shape[1], img.shape[2]), dtype=np.float32)\n",
    "            img = np.concatenate([img, padding], axis=0)\n",
    "\n",
    "        # Flatten spatial dims: (C, H*W)\n",
    "        pixels = img.reshape(bands, -1)\n",
    "        \n",
    "        channel_sum += pixels.sum(axis=1)\n",
    "        channel_sum_sq += (pixels ** 2).sum(axis=1)\n",
    "        pixel_num += pixels.shape[1]\n",
    "    \n",
    "    mean = channel_sum / pixel_num\n",
    "    std = np.sqrt(channel_sum_sq / pixel_num - mean ** 2)\n",
    "    \n",
    "    # Convert to torch tensor (C, 1, 1)\n",
    "    return mean, std\n",
    "\n",
    "def label_from_filename(fname: str) -> str:\n",
    "    return os.path.basename(fname).split(\"_\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "new_cell_73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Hyperspectral Dataset with Global Z-score Normalization\n",
    "    Resizes to TARGET_HW (64, 64)\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, file_list=None, target_bands=125, target_hw=(64,64), \n",
    "                 augment=False, mean=None, std=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.target_bands = target_bands\n",
    "        self.target_hw = target_hw\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Normalization Stats\n",
    "        self.mean = torch.tensor(mean).view(target_bands, 1, 1).float() if mean is not None else torch.zeros(target_bands, 1, 1)\n",
    "        self.std = torch.tensor(std).view(target_bands, 1, 1).float() if std is not None else torch.ones(target_bands, 1, 1)\n",
    "\n",
    "        if file_list is not None:\n",
    "            self.files = file_list\n",
    "        else:\n",
    "            self.files = sorted([f for f in os.listdir(img_dir) if f.lower().endswith((\".tif\", \".tiff\"))])\n",
    "        \n",
    "        # Label mapping\n",
    "        labels = sorted({label_from_filename(f) for f in self.files})\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(labels)}\n",
    "        self.idx_to_class = {i: c for c, i in self.class_to_idx.items()}\n",
    "        self.y = [self.class_to_idx[label_from_filename(f)] for f in self.files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        label = self.y[idx]\n",
    "        path = os.path.join(self.img_dir, fname)\n",
    "        \n",
    "        # Load\n",
    "        arr = tiff.imread(path).astype(np.float32)\n",
    "        \n",
    "        # Ensure (C, H, W)\n",
    "        if arr.ndim == 2: arr = arr[None, :, :]\n",
    "        elif arr.ndim == 3 and (arr.shape[-1] == self.target_bands or arr.shape[-1] < arr.shape[0]):\n",
    "             arr = np.transpose(arr, (2, 0, 1))\n",
    "             \n",
    "        # Fix bands\n",
    "        c = arr.shape[0]\n",
    "        if c > self.target_bands:\n",
    "            arr = arr[:self.target_bands]\n",
    "        elif c < self.target_bands:\n",
    "            pad = np.zeros((self.target_bands - c, arr.shape[1], arr.shape[2]), dtype=np.float32)\n",
    "            arr = np.concatenate([arr, pad], axis=0)\n",
    "\n",
    "        x = torch.from_numpy(arr)\n",
    "        \n",
    "        # Resize\n",
    "        if x.shape[1:] != self.target_hw:\n",
    "            x = x.unsqueeze(0)\n",
    "            x = F.interpolate(x, size=self.target_hw, mode=\"bilinear\", align_corners=False)\n",
    "            x = x.squeeze(0)\n",
    "\n",
    "        # Normalize (Global Z-score)\n",
    "        # Optional: Clip outlier pixels before norm? \n",
    "        # For baseline, standard (x-u)/s is fine. Could start by clipping to 0-1 range if raw data is huge.\n",
    "        # Assuming raw data is uint16-like but loaded as float.\n",
    "        x = (x - self.mean) / (self.std + 1e-8)\n",
    "        \n",
    "        # Augmentation\n",
    "        if self.augment:\n",
    "            if torch.rand(1) > 0.5: x = torch.flip(x, dims=[2])\n",
    "            if torch.rand(1) > 0.5: x = torch.flip(x, dims=[1])\n",
    "            k = torch.randint(0, 4, (1,)).item()\n",
    "            x = torch.rot90(x, k, dims=[1, 2])\n",
    "\n",
    "        return x, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "new_cell_28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating stats...\n",
      "Computing global stats per band...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:00<00:00, 965.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean[0:5]: [264.61113607 320.74743815 348.97407389 366.44861979 378.22931315]\n",
      "Std [0:5]: [330.00531389 347.41296516 352.80715128 355.75865957 360.99587943]\n",
      "Train: 480 | Val: 120\n",
      "Batch shape: torch.Size([32, 125, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Calculating stats...\")\n",
    "mean_stats, std_stats = compute_global_stats(HS_DIR, bands=TARGET_BANDS)\n",
    "print(\"Mean[0:5]:\", mean_stats[:5])\n",
    "print(\"Std [0:5]:\", std_stats[:5])\n",
    "\n",
    "# Split\n",
    "all_files = sorted([f for f in os.listdir(HS_DIR) if f.endswith(('.tif', '.tiff'))])\n",
    "labels = [label_from_filename(f) for f in all_files]\n",
    "indices = np.arange(len(all_files))\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    indices, test_size=VAL_RATIO, random_state=SEED, stratify=labels\n",
    ")\n",
    "\n",
    "train_files = [all_files[i] for i in train_idx]\n",
    "val_files = [all_files[i] for i in val_idx]\n",
    "\n",
    "train_ds = HSDataset(HS_DIR, train_files, TARGET_BANDS, TARGET_HW, augment=True, \n",
    "                     mean=mean_stats, std=std_stats)\n",
    "val_ds   = HSDataset(HS_DIR, val_files, TARGET_BANDS, TARGET_HW, augment=False, \n",
    "                     mean=mean_stats, std=std_stats)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Train: {len(train_ds)} | Val: {len(val_ds)}\")\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch shape:\", xb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "new_cell_19",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_ds.class_to_idx)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Adjust first conv for 125 channels\n",
    "old_conv = model.conv1\n",
    "model.conv1 = nn.Conv2d(TARGET_BANDS, old_conv.out_channels, \n",
    "                        kernel_size=old_conv.kernel_size, stride=old_conv.stride, \n",
    "                        padding=old_conv.padding, bias=False)\n",
    "\n",
    "# Init weights: average RGB weights and replicate\n",
    "with torch.no_grad():\n",
    "    model.conv1.weight[:] = old_conv.weight.mean(dim=1, keepdim=True).repeat(1, TARGET_BANDS, 1, 1)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "new_cell_45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from C:\\Users\\ADMIN\\_netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphucga150625\u001b[0m (\u001b[33mphucga15062005\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\Notebooks\\00_baseline\\wandb\\run-20260130_132546-hw3yflod</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/hw3yflod' target=\"_blank\">baseline_hs125_fixed</a></strong> to <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/hw3yflod' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/hw3yflod</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Acc: 0.4750 | Val Acc: 0.5000\n",
      "Saved best model: 0.5000\n",
      "Epoch 2 | Train Acc: 0.5896 | Val Acc: 0.4833\n",
      "Epoch 3 | Train Acc: 0.5958 | Val Acc: 0.6167\n",
      "Saved best model: 0.6167\n",
      "Epoch 4 | Train Acc: 0.6000 | Val Acc: 0.5583\n",
      "Epoch 5 | Train Acc: 0.6417 | Val Acc: 0.5417\n",
      "Epoch 6 | Train Acc: 0.6500 | Val Acc: 0.5833\n",
      "Epoch 7 | Train Acc: 0.6792 | Val Acc: 0.5417\n",
      "Epoch 8 | Train Acc: 0.6771 | Val Acc: 0.5000\n",
      "Epoch 9 | Train Acc: 0.6833 | Val Acc: 0.5583\n",
      "Epoch 10 | Train Acc: 0.6896 | Val Acc: 0.6333\n",
      "Saved best model: 0.6333\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▅▅▅▆▇████</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▁▂▁▁</td></tr><tr><td>val_acc</td><td>▂▁▇▅▄▆▄▂▅█</td></tr><tr><td>val_loss</td><td>█▆▃▁▁▂▄▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.68958</td></tr><tr><td>train_loss</td><td>0.65435</td></tr><tr><td>val_acc</td><td>0.63333</td></tr><tr><td>val_loss</td><td>0.82208</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline_hs125_fixed</strong> at: <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/hw3yflod' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/hw3yflod</a><br> View project at: <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260130_132546-hw3yflod\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"beyond-visible-spectrum\", name=\"baseline_hs125_fixed\")\n",
    "\n",
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss, train_acc = train_one_epoch(train_loader)\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), CKPT_PATH)\n",
    "        print(f\"Saved best model: {val_acc:.4f}\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "new_cell_57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_hs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class HSTestDataset(Dataset):\n",
    "    def __init__(self, img_dir, target_bands=125, target_hw=(64,64), mean=None, std=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.target_bands = target_bands\n",
    "        self.target_hw = target_hw\n",
    "        self.mean = torch.tensor(mean).view(target_bands, 1, 1).float() if mean is not None else torch.zeros(target_bands, 1, 1)\n",
    "        self.std = torch.tensor(std).view(target_bands, 1, 1).float() if std is not None else torch.ones(target_bands, 1, 1)\n",
    "        self.files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.tif', '.tiff'))])\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        path = os.path.join(self.img_dir, fname)\n",
    "        arr = tiff.imread(path).astype(np.float32)\n",
    "        if arr.ndim == 2: arr = arr[None, :, :]\n",
    "        elif arr.ndim == 3 and (arr.shape[-1] == self.target_bands or arr.shape[-1] < arr.shape[0]):\n",
    "             arr = np.transpose(arr, (2, 0, 1))\n",
    "        \n",
    "        c = arr.shape[0]\n",
    "        if c > self.target_bands: arr = arr[:self.target_bands]\n",
    "        elif c < self.target_bands:\n",
    "            pad = np.zeros((self.target_bands - c, arr.shape[1], arr.shape[2]), dtype=np.float32)\n",
    "            arr = np.concatenate([arr, pad], axis=0)\n",
    "            \n",
    "        x = torch.from_numpy(arr)\n",
    "        if x.shape[1:] != self.target_hw:\n",
    "            x = x.unsqueeze(0)\n",
    "            x = F.interpolate(x, size=self.target_hw, mode=\"bilinear\", align_corners=False)\n",
    "            x = x.squeeze(0)\n",
    "            \n",
    "        x = (x - self.mean) / (self.std + 1e-8)\n",
    "        return x, fname\n",
    "\n",
    "if os.path.exists(TEST_HS_DIR):\n",
    "    model.load_state_dict(torch.load(CKPT_PATH))\n",
    "    model.eval()\n",
    "    \n",
    "    test_ds = HSTestDataset(TEST_HS_DIR, TARGET_BANDS, TARGET_HW, mean=mean_stats, std=std_stats)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    preds = []\n",
    "    ids = []\n",
    "    class_names = [train_ds.idx_to_class[i] for i in range(num_classes)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, fname in tqdm(test_loader):\n",
    "            x = x.to(device)\n",
    "            out = model(x)\n",
    "            p_idx = out.argmax(1).cpu().numpy()\n",
    "            preds.extend([class_names[i] for i in p_idx])\n",
    "            ids.extend(fname)\n",
    "            \n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\"Id\": ids, \"Category\": preds})\n",
    "    df.to_csv(os.path.join(CHECKPOINT_DIR, \"submission_hs.csv\"), index=False)\n",
    "    print(\"Saved submission_hs.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
