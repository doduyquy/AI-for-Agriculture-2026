{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e1db667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tifffile as tiff\n",
    "import torchvision.models as models\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ===== PATHS =====\n",
    "HS_DIR = r\"D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\data\\raw\\train\\HS\"\n",
    "TEST_HS_DIR = r\"D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\data\\raw\\val\\HS\"\n",
    "CHECKPOINT_DIR = r\"D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\checkpoints\"\n",
    "CKPT_PATH = os.path.join(CHECKPOINT_DIR, \"best_hs125_resnet18.pth\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# ===== DATA SETTINGS =====\n",
    "TARGET_BANDS = 125\n",
    "TARGET_HW = (64, 64)       # Resizing to 64x64 for consistency\n",
    "\n",
    "# ===== SPLIT =====\n",
    "VAL_RATIO = 0.2\n",
    "SEED = 42\n",
    "\n",
    "# ===== TRAIN =====\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "WD = 1e-4\n",
    "NUM_WORKERS = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e4ac766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# ===== REPRODUCIBILITY =====\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "    \n",
    "    # Make PyTorch deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    # Set environment variable for CUDA\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    print(f\"✓ Random seed set to {seed} for reproducibility\")\n",
    "\n",
    "# Apply seed\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c996475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS classes: ['Health', 'Other', 'Rust']\n",
      "NUM_CLASSES = 3\n"
     ]
    }
   ],
   "source": [
    "prefixes = sorted({\n",
    "    fn.split(\"_\")[0]\n",
    "    for fn in os.listdir(HS_DIR)\n",
    "    if fn.endswith(\".tif\")\n",
    "})\n",
    "\n",
    "print(\"MS classes:\", prefixes)\n",
    "print(\"NUM_CLASSES =\", len(prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "new_cell_46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# ===============================================================\n",
    "# Robust CHW conversion  (dùng chung cho stats / train / test)\n",
    "# ===============================================================\n",
    "def ensure_chw(arr: np.ndarray, expected_bands: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Đảm bảo array có dạng (C, H, W).\n",
    "    Quy tắc:\n",
    "      - 2-D  -> (1, H, W)\n",
    "      - 3-D  -> so sánh shape[0] vs shape[2]:\n",
    "          * Nếu shape[2] == expected_bands            -> HWC, transpose\n",
    "          * Nếu shape[0] == expected_bands            -> đã CHW\n",
    "          * Nếu cả 2 != expected_bands                -> chọn dim nhỏ nhất làm C\n",
    "    \"\"\"\n",
    "    if arr.ndim == 2:\n",
    "        return arr[None, :, :]\n",
    "\n",
    "    if arr.ndim != 3:\n",
    "        raise ValueError(f\"Expected 2-D or 3-D array, got {arr.ndim}-D\")\n",
    "\n",
    "    d0, d1, d2 = arr.shape\n",
    "\n",
    "    # Trường hợp rõ ràng\n",
    "    if d2 == expected_bands and d0 != expected_bands:\n",
    "        return np.transpose(arr, (2, 0, 1))         # HWC -> CHW\n",
    "    if d0 == expected_bands and d2 != expected_bands:\n",
    "        return arr                                    # CHW rồi\n",
    "\n",
    "    # Cả 2 chiều == expected_bands (vuông): ưu tiên CHW (dim-0 = C)\n",
    "    if d0 == expected_bands and d2 == expected_bands:\n",
    "        return arr  # giữ nguyên, coi dim-0 là C\n",
    "\n",
    "    # Không chiều nào == expected_bands: chọn dim nhỏ nhất làm C\n",
    "    dims = [d0, d1, d2]\n",
    "    c_axis = int(np.argmin(dims))\n",
    "    if c_axis == 2:\n",
    "        return np.transpose(arr, (2, 0, 1))\n",
    "    elif c_axis == 1:\n",
    "        return np.transpose(arr, (1, 0, 2))\n",
    "    else:\n",
    "        return arr\n",
    "\n",
    "\n",
    "def fix_bands(arr: np.ndarray, target_bands: int) -> np.ndarray:\n",
    "    \"\"\"Cắt hoặc pad bands. Pad bằng mean spatial (tránh tín hiệu giả).\"\"\"\n",
    "    c = arr.shape[0]\n",
    "    if c > target_bands:\n",
    "        return arr[:target_bands]\n",
    "    if c < target_bands:\n",
    "        pad = np.repeat(\n",
    "            arr.mean(axis=0, keepdims=True),    # mean spatial\n",
    "            target_bands - c, axis=0\n",
    "        ).astype(arr.dtype)\n",
    "        return np.concatenate([arr, pad], axis=0)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def clip_per_band(x: torch.Tensor, ql=0.01, qh=0.99) -> torch.Tensor:\n",
    "    \"\"\"Clip mỗi band theo quantile q1/q99. Input/output: (C, H, W).\"\"\"\n",
    "    C = x.shape[0]\n",
    "    flat = x.view(C, -1)\n",
    "    lo = torch.quantile(flat, ql, dim=1).view(-1, 1, 1)\n",
    "    hi = torch.quantile(flat, qh, dim=1).view(-1, 1, 1)\n",
    "    return torch.clamp(x, lo, hi)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# Compute global stats  (match pipeline: ensure_chw → fix_bands\n",
    "#                         → resize → clip_per_band → accumulate)\n",
    "# ===============================================================\n",
    "@torch.no_grad()\n",
    "def compute_global_stats_matched(\n",
    "    img_dir,\n",
    "    file_list,\n",
    "    target_bands=125,\n",
    "    target_hw=(64, 64),\n",
    "    clip_q=(0.01, 0.99),\n",
    "    eps=1e-8,\n",
    "    max_files=None,\n",
    "):\n",
    "    ql, qh = clip_q\n",
    "    sum_c   = torch.zeros(target_bands, dtype=torch.float64)\n",
    "    sumsq_c = torch.zeros(target_bands, dtype=torch.float64)\n",
    "    count   = 0\n",
    "\n",
    "    files = file_list if max_files is None else file_list[:max_files]\n",
    "\n",
    "    for fname in tqdm(files, desc=\"Computing stats\"):\n",
    "        path = os.path.join(img_dir, fname)\n",
    "        arr = tiff.imread(path).astype(np.float32)\n",
    "\n",
    "        arr = ensure_chw(arr, target_bands)\n",
    "        arr = fix_bands(arr, target_bands)\n",
    "\n",
    "        x = torch.from_numpy(arr)  # (C, H, W)\n",
    "\n",
    "        # Resize\n",
    "        if x.shape[1:] != target_hw:\n",
    "            x = F.interpolate(x.unsqueeze(0), size=target_hw,\n",
    "                              mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "\n",
    "        # Clip per-band\n",
    "        x = clip_per_band(x, ql, qh)\n",
    "\n",
    "        # Accumulate\n",
    "        sum_c   += x.sum(dim=(1, 2), dtype=torch.float64)\n",
    "        sumsq_c += (x * x).sum(dim=(1, 2), dtype=torch.float64)\n",
    "        count   += x.shape[1] * x.shape[2]\n",
    "\n",
    "    mean = (sum_c / (count + eps)).to(torch.float32)\n",
    "    var  = (sumsq_c / (count + eps) - mean.double()**2).clamp_min(0.0).to(torch.float32)\n",
    "    std  = torch.sqrt(var + eps)\n",
    "\n",
    "    return mean.cpu().numpy(), std.cpu().numpy()\n",
    "\n",
    "\n",
    "def label_from_filename(fname: str) -> str:\n",
    "    return os.path.basename(fname).split(\"_\")[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c68a2322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tifffile as tiff\n",
    "\n",
    "TARGET_BANDS = 125\n",
    "\n",
    "def to_chw(arr: np.ndarray, expected_bands: int = 125) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Đưa ảnh về dạng (C,H,W) từ các dạng phổ biến:\n",
    "    - (H,W,C)\n",
    "    - (C,H,W)\n",
    "    - (H,W) (hiếm)\n",
    "    \"\"\"\n",
    "    if arr.ndim == 2:\n",
    "        # (H,W) -> (1,H,W)\n",
    "        return arr[None, :, :]\n",
    "\n",
    "    if arr.ndim != 3:\n",
    "        raise ValueError(f\"Unsupported ndim={arr.ndim}, shape={arr.shape}\")\n",
    "\n",
    "    a, b, c = arr.shape\n",
    "\n",
    "    # Nếu đang HWC và C khớp expected hoặc rất gần expected\n",
    "    if c == expected_bands or c == expected_bands + 1 or c == expected_bands - 1:\n",
    "        return np.transpose(arr, (2, 0, 1))  # HWC -> CHW\n",
    "\n",
    "    # Nếu đang CHW và C khớp expected hoặc gần expected\n",
    "    if a == expected_bands or a == expected_bands + 1 or a == expected_bands - 1:\n",
    "        return arr  # đã CHW\n",
    "\n",
    "    # Heuristic: chọn chiều nhỏ nhất làm channel (thường C nhỏ hơn H,W nếu ảnh lớn)\n",
    "    # Nhưng với case 32x32x125 thì C=125 lại lớn hơn 32 -> heuristic này không dùng được.\n",
    "    # Nên ta ưu tiên nếu có chiều đúng/near expected ở trên; nếu không có thì fallback:\n",
    "    # Nếu một chiều <= 256 và hai chiều còn lại bằng nhau (ví dụ 32x32x125), khả năng C là chiều còn lại.\n",
    "    if a == b and c <= 256:\n",
    "        # arr là (H,W,C)\n",
    "        return np.transpose(arr, (2, 0, 1))\n",
    "    if b == c and a <= 256:\n",
    "        # arr là (C,H,W) nhưng H=W?\n",
    "        return arr\n",
    "    if a == c and b <= 256:\n",
    "        # hiếm, nhưng vẫn xử lý\n",
    "        return np.transpose(arr, (1, 0, 2))  # -> (H,C,W) rồi sẽ lỗi; để an toàn, raise\n",
    "    # Nếu vẫn mơ hồ:\n",
    "    raise ValueError(f\"Ambiguous shape {arr.shape}: can't infer channel dim safely.\")\n",
    "\n",
    "def fix_bands_chw(x_chw: np.ndarray, target_bands: int = 125, pad_mode: str = \"mean\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    x_chw: (C,H,W) -> trả về (125,H,W)\n",
    "    - C > 125: cắt\n",
    "    - C == 126: cắt bỏ band cuối (thường là band thừa)\n",
    "    - C < 125: pad thêm band\n",
    "    \"\"\"\n",
    "    C, H, W = x_chw.shape\n",
    "\n",
    "    if C == target_bands:\n",
    "        return x_chw\n",
    "\n",
    "    if C > target_bands:\n",
    "        return x_chw[:target_bands, :, :]\n",
    "\n",
    "    # C < target_bands: pad\n",
    "    if pad_mode == \"mean\":\n",
    "        band_mean = x_chw.mean(axis=(1, 2), keepdims=True)  # (C,1,1)\n",
    "        pad_C = target_bands - C\n",
    "        # lặp mean của toàn ảnh theo từng band cuối cùng (hoặc dùng global mean chung)\n",
    "        # Cách đơn giản: dùng mean chung của toàn tensor\n",
    "        global_mean = x_chw.mean(keepdims=True)  # (1,1,1)\n",
    "        pad = np.repeat(global_mean, pad_C, axis=0)  # (pad_C,1,1) -> sẽ broadcast sai\n",
    "        # sửa cho đúng shape (pad_C,H,W)\n",
    "        pad = np.repeat(global_mean, pad_C, axis=0)\n",
    "        pad = np.repeat(pad, H, axis=1)\n",
    "        pad = np.repeat(pad, W, axis=2)\n",
    "        return np.concatenate([x_chw, pad], axis=0)\n",
    "\n",
    "    elif pad_mode == \"zero\":\n",
    "        pad_C = target_bands - C\n",
    "        pad = np.zeros((pad_C, H, W), dtype=x_chw.dtype)\n",
    "        return np.concatenate([x_chw, pad], axis=0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"pad_mode must be 'mean' or 'zero'\")\n",
    "\n",
    "def load_hs_as_125(path: str, target_hw=(64, 64), target_bands: int = 125) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Đọc tif -> CHW -> fix bands -> resize -> tensor float32\n",
    "    \"\"\"\n",
    "    arr = tiff.imread(path).astype(np.float32)\n",
    "\n",
    "    x = to_chw(arr, expected_bands=target_bands)          # (C,H,W)\n",
    "    x = fix_bands_chw(x, target_bands=target_bands)       # (125,H,W)\n",
    "\n",
    "    x = torch.from_numpy(x)                               # (C,H,W)\n",
    "    x = x.unsqueeze(0)                                    # (1,C,H,W)\n",
    "    x = F.interpolate(x, size=target_hw, mode=\"bilinear\", align_corners=False)\n",
    "    x = x.squeeze(0)                                      # (C,H,W)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "new_cell_73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Hyperspectral Dataset with Global Z-score Normalization.\n",
    "    Pipeline: load → ensure_chw → fix_bands → resize → clip_per_band → z-score → augment\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, file_list=None, target_bands=125, target_hw=(64, 64),\n",
    "                 augment=False, mean=None, std=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.target_bands = target_bands\n",
    "        self.target_hw = target_hw\n",
    "        self.augment = augment\n",
    "\n",
    "        # Normalization stats (per-band)\n",
    "        self.mean = (torch.tensor(mean).view(target_bands, 1, 1).float()\n",
    "                     if mean is not None else torch.zeros(target_bands, 1, 1))\n",
    "        self.std  = (torch.tensor(std).view(target_bands, 1, 1).float()\n",
    "                     if std is not None else torch.ones(target_bands, 1, 1))\n",
    "\n",
    "        if file_list is not None:\n",
    "            self.files = file_list\n",
    "        else:\n",
    "            self.files = sorted([f for f in os.listdir(img_dir)\n",
    "                                 if f.lower().endswith((\".tif\", \".tiff\"))])\n",
    "\n",
    "        # Label mapping\n",
    "        labels = sorted({label_from_filename(f) for f in self.files})\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(labels)}\n",
    "        self.idx_to_class = {i: c for c, i in self.class_to_idx.items()}\n",
    "        self.y = [self.class_to_idx[label_from_filename(f)] for f in self.files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        label = self.y[idx]\n",
    "        path  = os.path.join(self.img_dir, fname)\n",
    "\n",
    "        # 1. Load\n",
    "        arr = tiff.imread(path).astype(np.float32)\n",
    "\n",
    "        # 2. Ensure CHW (ƯU TIÊN case HWC = (32,32,125/126))\n",
    "        if arr.ndim != 3:\n",
    "            raise ValueError(f\"Invalid shape {arr.shape} | file={fname}\")\n",
    "\n",
    "        # Nếu là HWC (rất phổ biến với HS)\n",
    "        if arr.shape[2] in (125, 126):\n",
    "            arr = np.transpose(arr, (2, 0, 1))  # HWC -> CHW\n",
    "\n",
    "        # Nếu là CHW\n",
    "        elif arr.shape[0] in (125, 126):\n",
    "            pass  # đã đúng CHW\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot infer channel dim: {arr.shape} | file={fname}\")\n",
    "\n",
    "        # 3. Fix bands → LUÔN về 125\n",
    "        C, H, W = arr.shape\n",
    "\n",
    "        if C == 126:\n",
    "            arr = arr[:125]          # cắt band thừa\n",
    "        elif C == 125:\n",
    "            pass                     # ok\n",
    "        elif C < 125:\n",
    "            # pad bằng spatial mean (rất hiếm với dataset này)\n",
    "            pad_c = 125 - C\n",
    "            mean_band = arr.mean(axis=(1, 2), keepdims=True).mean(axis=0)\n",
    "            pad = np.repeat(mean_band, pad_c, axis=0)\n",
    "            arr = np.concatenate([arr, pad], axis=0)\n",
    "        else:\n",
    "            arr = arr[:125]\n",
    "\n",
    "        x = torch.from_numpy(arr)    # (125,H,W)\n",
    "        assert x.shape[0] == 125, f\"Band mismatch {x.shape} | file={fname}\"\n",
    "\n",
    "        # 4. Resize\n",
    "        if x.shape[1:] != self.target_hw:\n",
    "            x = F.interpolate(\n",
    "                x.unsqueeze(0),\n",
    "                size=self.target_hw,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            ).squeeze(0)\n",
    "\n",
    "        # 5. Clip per-band (LUÔN chạy)\n",
    "        x = clip_per_band(x, 0.01, 0.99)\n",
    "\n",
    "        # 6. Z-score normalize\n",
    "        x = (x - self.mean) / (self.std + 1e-8)\n",
    "\n",
    "        # 7. Augmentation (spatial only – OK cho HS)\n",
    "        if self.augment:\n",
    "            if torch.rand(1) > 0.5:\n",
    "                x = torch.flip(x, dims=[2])  # horizontal\n",
    "            if torch.rand(1) > 0.5:\n",
    "                x = torch.flip(x, dims=[1])  # vertical\n",
    "            k = torch.randint(0, 4, (1,)).item()\n",
    "            x = torch.rot90(x, k, dims=[1, 2])\n",
    "\n",
    "        return x, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25085bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Output bands after preprocessing ===\n",
      "125 577\n",
      "\n",
      "Non-125 or error samples: 0\n",
      "First 10: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "HS_DIR = r\"D:\\GitHub\\AI-for-Agriculture-2026\\dataset\\train\\HS\"  # sửa đúng path của bạn\n",
    "\n",
    "# Nếu bạn chưa có mean/std thì cho None để chỉ check shape\n",
    "ds = HSDataset(\n",
    "    img_dir=HS_DIR,\n",
    "    target_bands=125,\n",
    "    target_hw=(64, 64),\n",
    "    augment=False,\n",
    "    mean=None,   # hoặc mean_stats\n",
    "    std=None     # hoặc std_stats\n",
    ")\n",
    "\n",
    "counter = Counter()\n",
    "bad = []\n",
    "\n",
    "for i in range(len(ds)):\n",
    "    try:\n",
    "        x, y = ds[i]\n",
    "        counter[int(x.shape[0])] += 1\n",
    "        if x.shape[0] != 125:\n",
    "            bad.append((i, ds.files[i], tuple(x.shape)))\n",
    "    except Exception as e:\n",
    "        bad.append((i, ds.files[i], f\"ERROR: {e}\"))\n",
    "\n",
    "print(\"=== Output bands after preprocessing ===\")\n",
    "for k in sorted(counter.keys()):\n",
    "    print(k, counter[k])\n",
    "\n",
    "print(\"\\nNon-125 or error samples:\", len(bad))\n",
    "print(\"First 10:\", bad[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "new_cell_28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared split loaded from: D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\Notebooks\\split\\splits\n",
      "  Total aligned samples: 577\n",
      "  Train: 461 | Val: 116\n",
      "  ✓ All files exist in D:\\GitHub\\AI-for-Agriculture-2026\\dataset\\train\\HS\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# Load SHARED SPLIT (công bằng giữa RGB / MS / HS)\n",
    "# ============================================================\n",
    "SPLIT_DIR = r\"D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\Notebooks\\split\\splits\"\n",
    "\n",
    "df_master = pd.read_csv(os.path.join(SPLIT_DIR, \"samples_master.csv\"))\n",
    "train_idx_shared = np.load(os.path.join(SPLIT_DIR, \"train_idx.npy\"))\n",
    "val_idx_shared   = np.load(os.path.join(SPLIT_DIR, \"val_idx.npy\"))\n",
    "\n",
    "df_train = df_master.iloc[train_idx_shared].reset_index(drop=True)\n",
    "df_val   = df_master.iloc[val_idx_shared].reset_index(drop=True)\n",
    "\n",
    "# Lấy filename từ hs_path (basename, không phụ thuộc root)\n",
    "train_files = [os.path.basename(p) for p in df_train[\"hs_path\"]]\n",
    "val_files   = [os.path.basename(p) for p in df_val[\"hs_path\"]]\n",
    "\n",
    "print(f\"Shared split loaded from: {SPLIT_DIR}\")\n",
    "print(f\"  Total aligned samples: {len(df_master)}\")\n",
    "print(f\"  Train: {len(train_files)} | Val: {len(val_files)}\")\n",
    "\n",
    "# Verify files exist in HS_DIR\n",
    "missing = [f for f in train_files + val_files if not os.path.exists(os.path.join(HS_DIR, f))]\n",
    "if missing:\n",
    "    print(f\"  ⚠️ Missing {len(missing)} files! First 5: {missing[:5]}\")\n",
    "else:\n",
    "    print(f\"  ✓ All files exist in {HS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1050c6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating stats (TRAIN only)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing stats:   0%|          | 0/461 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing stats: 100%|██████████| 461/461 [00:32<00:00, 14.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean[0:5]: [581.4699  626.0004  649.4432  662.0995  669.45667]\n",
      "Std [0:5]: [1885.0404 1878.6755 1874.7366 1871.2903 1868.411 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 3) Tính stats trên TRAIN\n",
    "# =========================\n",
    "print(\"Calculating stats (TRAIN only)...\")\n",
    "mean_stats, std_stats = compute_global_stats_matched(\n",
    "    HS_DIR, train_files,\n",
    "    target_bands=TARGET_BANDS,\n",
    "    target_hw=TARGET_HW,\n",
    "    clip_q=(0.01, 0.99)\n",
    ")\n",
    "\n",
    "print(\"Mean[0:5]:\", mean_stats[:5])\n",
    "print(\"Std [0:5]:\", std_stats[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70f05448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 461 | Val: 116\n",
      "Batch shape: torch.Size([32, 125, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4) Dataset + DataLoader\n",
    "# =========================\n",
    "train_ds = HSDataset(\n",
    "    HS_DIR, file_list=train_files,\n",
    "    target_bands=TARGET_BANDS, target_hw=TARGET_HW,\n",
    "    augment=True, mean=mean_stats, std=std_stats\n",
    ")\n",
    "\n",
    "val_ds = HSDataset(\n",
    "    HS_DIR, file_list=val_files,\n",
    "    target_bands=TARGET_BANDS, target_hw=TARGET_HW,\n",
    "    augment=False, mean=mean_stats, std=std_stats\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Train: {len(train_ds)} | Val: {len(val_ds)}\")\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch shape:\", xb.shape)  # kỳ vọng: [B, 125, 64, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34213c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     x, y = dataset[i]\n",
    "#     print(x.shape, x.min().item(), x.max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "503d4a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape : torch.Size([32, 125, 64, 64])\n",
      "dtype       : torch.float32\n",
      "Min / Max   : -0.9381 / 1.7339\n",
      "Mean / Std  : -0.0584 / 0.3752\n",
      "Còn giá trị thô (>1000)? KHÔNG ✓\n",
      "\n",
      "Trước clip  : min=527.5  max=3091.5\n",
      "Sau clip    : min=581.2  max=3010.9\n",
      "Sau z-score : min=-0.32  max=0.69\n"
     ]
    }
   ],
   "source": [
    "# ====== Sanity check: giá trị 65535 (uint16 max) không nên còn sau pipeline ======\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(f\"Batch shape : {xb.shape}\")          # expect [B, 125, 64, 64]\n",
    "print(f\"dtype       : {xb.dtype}\")\n",
    "print(f\"Min / Max   : {xb.min():.4f} / {xb.max():.4f}\")\n",
    "print(f\"Mean / Std  : {xb.mean():.4f} / {xb.std():.4f}\")\n",
    "\n",
    "has_raw = (xb.abs() > 1000).any().item()\n",
    "print(f\"Còn giá trị thô (>1000)? {'CÓ ⚠️' if has_raw else 'KHÔNG ✓'}\")\n",
    "\n",
    "# Check 1 sample riêng (trước z-score) để xác nhận clip hoạt động\n",
    "sample_arr = tiff.imread(os.path.join(HS_DIR, train_files[0])).astype(np.float32)\n",
    "sample_arr = ensure_chw(sample_arr, TARGET_BANDS)\n",
    "sample_arr = fix_bands(sample_arr, TARGET_BANDS)\n",
    "xs = torch.from_numpy(sample_arr)\n",
    "if xs.shape[1:] != TARGET_HW:\n",
    "    xs = F.interpolate(xs.unsqueeze(0), size=TARGET_HW, mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "print(f\"\\nTrước clip  : min={xs.min():.1f}  max={xs.max():.1f}\")\n",
    "xs = clip_per_band(xs, 0.01, 0.99)\n",
    "print(f\"Sau clip    : min={xs.min():.1f}  max={xs.max():.1f}\")\n",
    "xs = (xs - torch.tensor(mean_stats).view(-1,1,1)) / (torch.tensor(std_stats).view(-1,1,1) + 1e-8)\n",
    "print(f\"Sau z-score : min={xs.min():.2f}  max={xs.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "new_cell_19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_ds.class_to_idx)\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Adjust first conv for 125 channels\n",
    "old_conv = model.conv1\n",
    "model.conv1 = nn.Conv2d(TARGET_BANDS, old_conv.out_channels, \n",
    "                        kernel_size=old_conv.kernel_size, stride=old_conv.stride, \n",
    "                        padding=old_conv.padding, bias=False)\n",
    "\n",
    "# Init weights: average RGB weights and replicate\n",
    "with torch.no_grad():\n",
    "    model.conv1.weight[:] = old_conv.weight.mean(dim=1, keepdim=True).repeat(1, TARGET_BANDS, 1, 1)\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Move entire model to GPU after all modifications\n",
    "model = model.to(device)\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56a98ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from collections import Counter\n",
    "# import tifffile as tiff\n",
    "# import numpy as np\n",
    "\n",
    "# HS_DIR = r\"D:\\GitHub\\AI-for-Agriculture-2026\\dataset\\train\\HS\"  # sửa lại đúng path của bạn\n",
    "\n",
    "# def infer_num_bands(arr: np.ndarray) -> int:\n",
    "#     if arr.ndim != 3:\n",
    "#         return -1  # lỗi shape\n",
    "#     # ưu tiên nếu có chiều 125/126 rõ ràng\n",
    "#     if arr.shape[2] in (125, 126):\n",
    "#         return arr.shape[2]       # HWC\n",
    "#     if arr.shape[0] in (125, 126):\n",
    "#         return arr.shape[0]       # CHW\n",
    "#     # fallback: lấy chiều lớn nhất làm C (case 32x32x125 -> 125 là lớn nhất)\n",
    "#     return max(arr.shape)\n",
    "\n",
    "# band_counter = Counter()\n",
    "# shape_counter = Counter()\n",
    "# bad_files = []\n",
    "\n",
    "# for fn in os.listdir(HS_DIR):\n",
    "#     if not fn.lower().endswith((\".tif\", \".tiff\")):\n",
    "#         continue\n",
    "#     path = os.path.join(HS_DIR, fn)\n",
    "#     try:\n",
    "#         arr = tiff.imread(path)\n",
    "#         shape_counter[str(arr.shape)] += 1\n",
    "#         b = infer_num_bands(arr)\n",
    "#         band_counter[b] += 1\n",
    "#     except Exception as e:\n",
    "#         bad_files.append((fn, str(e)))\n",
    "\n",
    "# print(\"=== Band counts ===\")\n",
    "# for k in sorted(band_counter.keys()):\n",
    "#     print(f\"{k}: {band_counter[k]}\")\n",
    "\n",
    "# print(\"\\n=== Top shapes ===\")\n",
    "# for s, c in shape_counter.most_common(10):\n",
    "#     print(f\"{s}: {c}\")\n",
    "\n",
    "# if bad_files:\n",
    "#     print(\"\\n=== Bad files (first 10) ===\")\n",
    "#     for x in bad_files[:10]:\n",
    "#         print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64723bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 125, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "x, y = batch\n",
    "print(\"Batch shape:\", x.shape)  # phải là [B, 125, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5168b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# # 1) Tạo INSTANCE dataset (đúng như bạn dùng để train)\n",
    "# dataset = HSDataset(\n",
    "#     img_dir=HS_DIR,          # ví dụ: r\"D:\\GitHub\\AI-for-Agriculture-2026\\dataset\\train\\HS\"\n",
    "#     files=train_files,       # nếu bạn có split; nếu không thì bỏ tham số này\n",
    "#     mean=mean_stats,         # tensor/np shape (125,) hoặc (125,1,1) tuỳ bạn thiết kế\n",
    "#     std=std_stats,\n",
    "#     target_bands=125,\n",
    "#     target_hw=(64,64),\n",
    "#     augment=False            # để check ổn định\n",
    "# )\n",
    "\n",
    "# counter = Counter()\n",
    "# bad = []\n",
    "\n",
    "# for i in range(len(dataset)):\n",
    "#     try:\n",
    "#         x, y = dataset[i]  # <-- đúng: index instance\n",
    "#         counter[int(x.shape[0])] += 1\n",
    "#         if x.shape[0] != 125:\n",
    "#             bad.append((i, dataset.files[i], tuple(x.shape)))\n",
    "#     except Exception as e:\n",
    "#         bad.append((i, dataset.files[i], f\"ERROR: {e}\"))\n",
    "\n",
    "# print(\"=== Output bands after preprocessing ===\")\n",
    "# for k in sorted(counter.keys()):\n",
    "#     print(k, counter[k])\n",
    "\n",
    "# print(\"\\nNon-125 or error samples:\", len(bad))\n",
    "# print(\"First 10:\", bad[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6b73e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "new_cell_45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\Notebooks\\00_baseline\\wandb\\run-20260208_154322-nxmmfwus</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/nxmmfwus' target=\"_blank\">baseline_hs125_resnet</a></strong> to <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/nxmmfwus' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/nxmmfwus</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Acc: 0.4512 | Val Acc: 0.5603\n",
      "Saved best model: 0.5603\n",
      "Epoch 2 | Train Acc: 0.5792 | Val Acc: 0.6034\n",
      "Saved best model: 0.6034\n",
      "Epoch 3 | Train Acc: 0.6703 | Val Acc: 0.6121\n",
      "Saved best model: 0.6121\n",
      "Epoch 4 | Train Acc: 0.6790 | Val Acc: 0.5862\n",
      "Epoch 5 | Train Acc: 0.6855 | Val Acc: 0.5776\n",
      "Epoch 6 | Train Acc: 0.7375 | Val Acc: 0.5690\n",
      "Epoch 7 | Train Acc: 0.7852 | Val Acc: 0.5690\n",
      "Epoch 8 | Train Acc: 0.7722 | Val Acc: 0.5517\n",
      "Epoch 9 | Train Acc: 0.7679 | Val Acc: 0.5690\n",
      "Epoch 10 | Train Acc: 0.8265 | Val Acc: 0.5862\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▃▅▅▅▆▇▇▇█</td></tr><tr><td>train_loss</td><td>█▆▄▄▄▃▂▂▂▁</td></tr><tr><td>val_acc</td><td>▂▇█▅▄▃▃▁▃▅</td></tr><tr><td>val_loss</td><td>█▄▁▃▂▄▆▆▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.82646</td></tr><tr><td>train_loss</td><td>0.4231</td></tr><tr><td>val_acc</td><td>0.58621</td></tr><tr><td>val_loss</td><td>0.99802</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline_hs125_resnet</strong> at: <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/nxmmfwus' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum/runs/nxmmfwus</a><br> View project at: <a href='https://wandb.ai/phucga15062005/beyond-visible-spectrum' target=\"_blank\">https://wandb.ai/phucga15062005/beyond-visible-spectrum</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260208_154322-nxmmfwus\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "wandb.init(project=\"beyond-visible-spectrum\", name=\"baseline_hs125_resnet\")\n",
    "\n",
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss, train_acc = train_one_epoch(train_loader)\n",
    "    val_loss, val_acc = evaluate(val_loader)\n",
    "    scheduler.step(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    wandb.log({\"train_loss\": train_loss, \"train_acc\": train_acc, \"val_loss\": val_loss, \"val_acc\": val_acc})\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), CKPT_PATH)\n",
    "        print(f\"Saved best model: {val_acc:.4f}\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb84f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from: D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\checkpoints\\best_hs125_resnet18.pth\n",
      "Model on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint for evaluation\n",
    "model.load_state_dict(torch.load(CKPT_PATH, map_location=device, weights_only=True))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"Loaded best model from: {CKPT_PATH}\")\n",
    "\n",
    "print(f\"Model on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85c271f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:     0.6121\n",
      "F1-macro:     0.6157\n",
      "F1-weighted:  0.6107\n",
      "\n",
      "Confusion Matrix (rows=true, cols=pred):\n",
      " [[19  4 16]\n",
      " [ 3 32  2]\n",
      " [18  2 20]]\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Health     0.4750    0.4872    0.4810        39\n",
      "       Other     0.8421    0.8649    0.8533        37\n",
      "        Rust     0.5263    0.5000    0.5128        40\n",
      "\n",
      "    accuracy                         0.6121       116\n",
      "   macro avg     0.6145    0.6173    0.6157       116\n",
      "weighted avg     0.6098    0.6121    0.6107       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, accuracy_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_preds_labels(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.append(preds.detach().cpu().numpy())\n",
    "        all_labels.append(y.detach().cpu().numpy())\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# 1) Lấy dự đoán + nhãn thật\n",
    "preds, labels = get_preds_labels(model, val_loader, device)\n",
    "\n",
    "# 2) Accuracy + F1\n",
    "acc = accuracy_score(labels, preds)\n",
    "f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "f1_weighted = f1_score(labels, preds, average=\"weighted\")\n",
    "\n",
    "print(f\"Accuracy:     {acc:.4f}\")\n",
    "print(f\"F1-macro:     {f1_macro:.4f}\")\n",
    "print(f\"F1-weighted:  {f1_weighted:.4f}\")\n",
    "\n",
    "# 3) Confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(\"\\nConfusion Matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "# 4) Report theo từng lớp\n",
    "# Nếu bạn có mapping idx_to_class thì in tên lớp cho đẹp\n",
    "if hasattr(val_loader.dataset, \"idx_to_class\"):\n",
    "    target_names = [val_loader.dataset.idx_to_class[i] for i in range(len(val_loader.dataset.idx_to_class))]\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(labels, preds, target_names=target_names, digits=4))\n",
    "else:\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(labels, preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e814c1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names=None, title=\"Confusion Matrix\"):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm, interpolation=\"nearest\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(cm))\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(len(cm))]\n",
    "    plt.xticks(tick_marks, class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    thresh = cm.max() / 2.0 if cm.max() > 0 else 0.5\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], \"d\"),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class_names = None\n",
    "if hasattr(val_loader.dataset, \"idx_to_class\"):\n",
    "    class_names = [val_loader.dataset.idx_to_class[i] for i in range(len(val_loader.dataset.idx_to_class))]\n",
    "\n",
    "plot_confusion_matrix(cm, class_names=class_names, title=\"Val Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "new_cell_57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: D:\\HocTap\\NCKH_ThayDoNhuTai\\Challenges\\checkpoints\\best_hs125_resnet18.pth\n",
      "Model on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 10/10 [00:09<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_hs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class HSTestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Test dataset — pipeline PHẢI khớp HSDataset (trừ augment & label):\n",
    "    load → ensure_chw → fix_bands → resize → clip_per_band → z-score\n",
    "    \"\"\"\n",
    "    def __init__(self, img_dir, target_bands=125, target_hw=(64, 64), mean=None, std=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.target_bands = target_bands\n",
    "        self.target_hw = target_hw\n",
    "        self.mean = (torch.tensor(mean).view(target_bands, 1, 1).float()\n",
    "                     if mean is not None else torch.zeros(target_bands, 1, 1))\n",
    "        self.std  = (torch.tensor(std).view(target_bands, 1, 1).float()\n",
    "                     if std is not None else torch.ones(target_bands, 1, 1))\n",
    "        self.files = sorted([f for f in os.listdir(img_dir)\n",
    "                             if f.lower().endswith(('.tif', '.tiff'))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.files[idx]\n",
    "        path  = os.path.join(self.img_dir, fname)\n",
    "\n",
    "        # 1. Load\n",
    "        arr = tiff.imread(path).astype(np.float32)\n",
    "\n",
    "        # 2. Ensure CHW  (cùng hàm với train)\n",
    "        arr = ensure_chw(arr, self.target_bands)\n",
    "\n",
    "        # 3. Fix bands  (pad spatial-mean, KHÔNG pad 0)\n",
    "        arr = fix_bands(arr, self.target_bands)\n",
    "\n",
    "        x = torch.from_numpy(arr)  # (C, H, W)\n",
    "\n",
    "        # 4. Resize\n",
    "        if x.shape[1:] != self.target_hw:\n",
    "            x = F.interpolate(x.unsqueeze(0), size=self.target_hw,\n",
    "                              mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "\n",
    "        # 5. Clip per-band  (TRƯỚC ĐÂY BỊ THIẾU → khiến z-score sai)\n",
    "        x = clip_per_band(x, 0.01, 0.99)\n",
    "\n",
    "        # 6. Z-score normalize\n",
    "        x = (x - self.mean) / (self.std + 1e-8)\n",
    "\n",
    "        return x, fname\n",
    "\n",
    "\n",
    "if os.path.exists(TEST_HS_DIR):\n",
    "    # Load best checkpoint for test prediction\n",
    "    model.load_state_dict(torch.load(CKPT_PATH, map_location=device, weights_only=True))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Loaded checkpoint: {CKPT_PATH}\")\n",
    "    print(f\"Model on device: {next(model.parameters()).device}\")\n",
    "\n",
    "    test_ds = HSTestDataset(TEST_HS_DIR, TARGET_BANDS, TARGET_HW,\n",
    "                            mean=mean_stats, std=std_stats)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    preds = []\n",
    "    ids = []\n",
    "    class_names = [train_ds.idx_to_class[i] for i in range(num_classes)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, fname in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            x = x.to(device)\n",
    "            out = model(x)\n",
    "            p_idx = out.argmax(1).cpu().numpy()\n",
    "            preds.extend([class_names[i] for i in p_idx])\n",
    "            ids.extend(fname)\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.DataFrame({\"Id\": ids, \"Category\": preds})\n",
    "\n",
    "    df.to_csv(os.path.join(CHECKPOINT_DIR, \"submission_hs.csv\"), index=False)    \n",
    "    print(\"Saved submission_hs.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ocr311)",
   "language": "python",
   "name": "ocr311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
